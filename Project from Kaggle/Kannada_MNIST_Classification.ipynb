{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8RbNzt6gHNbw"
   },
   "source": [
    "# **CS985 and CS987 Assignment 2 : Kannada-MNIST Classification Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MmUpFDwhEQzE"
   },
   "source": [
    "#### **Team's name: Group O**\n",
    "\n",
    "-   Chavinpat Naimee (201976778)\n",
    "-   Chinnakrit Nitipornsri (201964692)\n",
    "-   Kittanon Na Nakhon (201964002)\n",
    "-   Nuallahong Silaung (201986533)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bs-u4oMzhU7s"
   },
   "source": [
    "Open in Google Colab [Click here](https://drive.google.com/file/d/1OmYWDHnR2-8Hyo19Y_pdIXcR6JFJ6Lui/view?usp=sharing) or https://drive.google.com/file/d/1OmYWDHnR2-8Hyo19Y_pdIXcR6JFJ6Lui/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZzS7tuJGHNb3"
   },
   "source": [
    "## **1. The Objective :**\n",
    "The main analysis of this project is to classify images of hand written digits in the data coming from Kannada, which is a language spoken predominantly by people of Karnataka in southwestern India. (The data comes from Classification problem: https://www.kaggle.com/t/e224c14938ae4525a2ff60b9cb846de6) \n",
    "\n",
    "## **2. Meaning of Dataset :**\n",
    "This dataset consists of two datasets of the train and test dataset, which indicates the data of 60,000 rows 786 columns and 10,000 rows 786 columns respectively. However, the missing column of the test dataset is \"label\" column, which is the key finding of this project.\n",
    "\n",
    "\n",
    "## **3. Overall analysis :**\n",
    "\n",
    "> ### **3.1 Data Exploding**\n",
    "\n",
    "> ### **3.2 Prepare the Data for Deep Learning Algorithms**\n",
    "- Preparing data\n",
    "- Spliting the train data and segment test data\n",
    "- Normalising data\n",
    "- Showing image data\n",
    "\n",
    "> ### **3.3 Deep Learning Analysis Models**\n",
    "- Creating a Fully Connected Neural Network model\n",
    "- Improving a Fully Connected Neural Network model\n",
    "- The Convolutional Neural Network (CNN)\n",
    "- Improving the accuracy by using Data Augmentation technique\n",
    "- CNN Architecture Improvement\n",
    "\n",
    "## **4. Conclusion**\n",
    "## **5. References**\n",
    "\n",
    "> #### **Required packages**\n",
    "- from google.colab import files\n",
    "- import numpy as np\n",
    "- import pandas as pd\n",
    "- import matplotlib.pyplot as plt\n",
    "- import sklearn\n",
    "- from sklearn import model_selection, preprocessing\n",
    "- from sklearn.preprocessing import scale\n",
    "- from sklearn.model_selection import train_test_split\n",
    "- import tensorflow as tf\n",
    "- from tensorflow import keras\n",
    "- from keras.models import Sequential, load_model\n",
    "- from keras.layers import Dense, Dropout, Flatten\n",
    "- from keras.layers import Conv2D, MaxPooling2D\n",
    "- from keras import backend as K\n",
    "- from tensorflow.keras.optimizers import RMSprop,Adam,Adadelta,SGD\n",
    "- from keras.preprocessing.image import ImageDataGenerator\n",
    "- from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Em0xkQU-TzPI"
   },
   "source": [
    "# **3. Overall analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mmpZ6m59HNb5"
   },
   "source": [
    "## **3.1 Data Exploding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BRxmdZm3HNb7"
   },
   "source": [
    "#### **3.1.1 Starting with import libraries and datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XXT4SlGkHNb8"
   },
   "source": [
    "##### ***In term of running on Google colab***\n",
    "\n",
    "If you use Google colab to run this code, you need the API from Kaggle before run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "6bo5uokzHNb9",
    "outputId": "a4af7333-40a6-4a53-dff7-db10dd4cf92e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-cd736bea-3e11-4e69-9463-e73ff0ef5d21\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-cd736bea-3e11-4e69-9463-e73ff0ef5d21\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kaggle.json': b'{\"username\":\"chinnakritn\",\"key\":\"1e82857533b2b1c8622562ac08a53ea1\"}'}"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "V1sd-frgHNcB"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/kaggle.json\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ktBJrE4RIyBb"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "F8aTBavy4inI"
   },
   "outputs": [],
   "source": [
    "#!python3 -m pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Ol4CExAtHNcF",
    "outputId": "6144690c-796f-42d2-c06d-e3752717c1a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
      "Downloading test.csv.zip to /content\n",
      "  0% 0.00/1.79M [00:00<?, ?B/s]\n",
      "100% 1.79M/1.79M [00:00<00:00, 58.9MB/s]\n",
      "Downloading training.csv.zip to /content\n",
      " 60% 6.00M/9.92M [00:00<00:00, 10.7MB/s]\n",
      "100% 9.92M/9.92M [00:00<00:00, 18.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c cs98x-kannada-mnist --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4_f5sQhd5J6L"
   },
   "outputs": [],
   "source": [
    "import numpy as np   # Implemennts milti-dimensional array and matrices\n",
    "import pandas as pd  # for data manipulation and analysis\n",
    "import tensorflow as tf # for create deep learning models\n",
    "from tensorflow import keras # implementation of the Keras API specification\n",
    "# load training & test datasets\n",
    "train = pd.read_csv(\"/content/training.csv.zip\")\n",
    "test = pd.read_csv(\"/content/test.csv.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qUT0_c-bHNcI"
   },
   "source": [
    "##### ***In term of running on Jupyter Notebook***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "X_8N7PVmzs1I"
   },
   "outputs": [],
   "source": [
    "import numpy as np   # Implemennts milti-dimensional array and matrices\n",
    "import pandas as pd  # for data manipulation and analysis\n",
    "import tensorflow as tf # for create deep learning models\n",
    "from tensorflow import keras # implementation of the Keras API specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "s9861XB5IyBl"
   },
   "outputs": [],
   "source": [
    "# To check version of tensorflow and keras\n",
    "print('Tensorflow version',tf.__version__)\n",
    "print('Keras version',keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_k8NXkhEmiJc"
   },
   "outputs": [],
   "source": [
    "# Read training & test datasets by pandas package\n",
    "train = pd.read_csv(\"./cs98x-kannada-mnist/training.csv\")\n",
    "test = pd.read_csv(\"./cs98x-kannada-mnist/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xKM-t3CAHNcQ"
   },
   "source": [
    "#### **3.1.2 Providing an overview of information of both datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "vm0oUAgtoc95",
    "outputId": "ff04ffda-6664-4bd0-da42-4871f50c4059"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>pixel11</th>\n",
       "      <th>pixel12</th>\n",
       "      <th>pixel13</th>\n",
       "      <th>pixel14</th>\n",
       "      <th>pixel15</th>\n",
       "      <th>pixel16</th>\n",
       "      <th>pixel17</th>\n",
       "      <th>pixel18</th>\n",
       "      <th>pixel19</th>\n",
       "      <th>pixel20</th>\n",
       "      <th>pixel21</th>\n",
       "      <th>pixel22</th>\n",
       "      <th>pixel23</th>\n",
       "      <th>pixel24</th>\n",
       "      <th>pixel25</th>\n",
       "      <th>pixel26</th>\n",
       "      <th>pixel27</th>\n",
       "      <th>pixel28</th>\n",
       "      <th>pixel29</th>\n",
       "      <th>pixel30</th>\n",
       "      <th>pixel31</th>\n",
       "      <th>pixel32</th>\n",
       "      <th>pixel33</th>\n",
       "      <th>pixel34</th>\n",
       "      <th>pixel35</th>\n",
       "      <th>pixel36</th>\n",
       "      <th>pixel37</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel744</th>\n",
       "      <th>pixel745</th>\n",
       "      <th>pixel746</th>\n",
       "      <th>pixel747</th>\n",
       "      <th>pixel748</th>\n",
       "      <th>pixel749</th>\n",
       "      <th>pixel750</th>\n",
       "      <th>pixel751</th>\n",
       "      <th>pixel752</th>\n",
       "      <th>pixel753</th>\n",
       "      <th>pixel754</th>\n",
       "      <th>pixel755</th>\n",
       "      <th>pixel756</th>\n",
       "      <th>pixel757</th>\n",
       "      <th>pixel758</th>\n",
       "      <th>pixel759</th>\n",
       "      <th>pixel760</th>\n",
       "      <th>pixel761</th>\n",
       "      <th>pixel762</th>\n",
       "      <th>pixel763</th>\n",
       "      <th>pixel764</th>\n",
       "      <th>pixel765</th>\n",
       "      <th>pixel766</th>\n",
       "      <th>pixel767</th>\n",
       "      <th>pixel768</th>\n",
       "      <th>pixel769</th>\n",
       "      <th>pixel770</th>\n",
       "      <th>pixel771</th>\n",
       "      <th>pixel772</th>\n",
       "      <th>pixel773</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 786 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  pixel0  pixel1  ...  pixel780  pixel781  pixel782  pixel783\n",
       "0   1      0       0       0  ...         0         0         0         0\n",
       "1   2      1       0       0  ...         0         0         0         0\n",
       "2   3      2       0       0  ...         0         0         0         0\n",
       "3   4      3       0       0  ...         0         0         0         0\n",
       "4   5      4       0       0  ...         0         0         0         0\n",
       "5   6      5       0       0  ...         0         0         0         0\n",
       "6   7      6       0       0  ...         0         0         0         0\n",
       "7   8      7       0       0  ...         0         0         0         0\n",
       "8   9      8       0       0  ...         0         0         0         0\n",
       "9  10      9       0       0  ...         0         0         0         0\n",
       "\n",
       "[10 rows x 786 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To show 10 first order of train dataset\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "pMtXOJYmHNcW",
    "outputId": "ae5aa862-0e57-4309-cbe0-8fbf5973aa4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 786)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print shape of train set\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "sXah6zzyHNcZ",
    "outputId": "1e4215d7-1f20-42e3-855a-8cbe5925e475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Columns: 786 entries, id to pixel783\n",
      "dtypes: int64(786)\n",
      "memory usage: 359.8 MB\n"
     ]
    }
   ],
   "source": [
    "# To get Number of columns, Index, Datatype and Memory information\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "nKCb_rTyHNcc",
    "outputId": "bb7b5c48-d0e5-4409-e103-de369a60c9af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "label       0\n",
       "pixel0      0\n",
       "pixel1      0\n",
       "pixel2      0\n",
       "           ..\n",
       "pixel779    0\n",
       "pixel780    0\n",
       "pixel781    0\n",
       "pixel782    0\n",
       "pixel783    0\n",
       "Length: 786, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find and count 'NaN' in datasat\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "0Q7QiPB-pBE6",
    "outputId": "f774bfca-d59e-4f74-e701-9757a1f84d68"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>pixel11</th>\n",
       "      <th>pixel12</th>\n",
       "      <th>pixel13</th>\n",
       "      <th>pixel14</th>\n",
       "      <th>pixel15</th>\n",
       "      <th>pixel16</th>\n",
       "      <th>pixel17</th>\n",
       "      <th>pixel18</th>\n",
       "      <th>pixel19</th>\n",
       "      <th>pixel20</th>\n",
       "      <th>pixel21</th>\n",
       "      <th>pixel22</th>\n",
       "      <th>pixel23</th>\n",
       "      <th>pixel24</th>\n",
       "      <th>pixel25</th>\n",
       "      <th>pixel26</th>\n",
       "      <th>pixel27</th>\n",
       "      <th>pixel28</th>\n",
       "      <th>pixel29</th>\n",
       "      <th>pixel30</th>\n",
       "      <th>pixel31</th>\n",
       "      <th>pixel32</th>\n",
       "      <th>pixel33</th>\n",
       "      <th>pixel34</th>\n",
       "      <th>pixel35</th>\n",
       "      <th>pixel36</th>\n",
       "      <th>pixel37</th>\n",
       "      <th>pixel38</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel744</th>\n",
       "      <th>pixel745</th>\n",
       "      <th>pixel746</th>\n",
       "      <th>pixel747</th>\n",
       "      <th>pixel748</th>\n",
       "      <th>pixel749</th>\n",
       "      <th>pixel750</th>\n",
       "      <th>pixel751</th>\n",
       "      <th>pixel752</th>\n",
       "      <th>pixel753</th>\n",
       "      <th>pixel754</th>\n",
       "      <th>pixel755</th>\n",
       "      <th>pixel756</th>\n",
       "      <th>pixel757</th>\n",
       "      <th>pixel758</th>\n",
       "      <th>pixel759</th>\n",
       "      <th>pixel760</th>\n",
       "      <th>pixel761</th>\n",
       "      <th>pixel762</th>\n",
       "      <th>pixel763</th>\n",
       "      <th>pixel764</th>\n",
       "      <th>pixel765</th>\n",
       "      <th>pixel766</th>\n",
       "      <th>pixel767</th>\n",
       "      <th>pixel768</th>\n",
       "      <th>pixel769</th>\n",
       "      <th>pixel770</th>\n",
       "      <th>pixel771</th>\n",
       "      <th>pixel772</th>\n",
       "      <th>pixel773</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>60009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>60010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  pixel0  pixel1  pixel2  ...  pixel780  pixel781  pixel782  pixel783\n",
       "0  60001       0       0       0  ...         0         0         0         0\n",
       "1  60002       0       0       0  ...         0         0         0         0\n",
       "2  60003       0       0       0  ...         0         0         0         0\n",
       "3  60004       0       0       0  ...         0         0         0         0\n",
       "4  60005       0       0       0  ...         0         0         0         0\n",
       "5  60006       0       0       0  ...         0         0         0         0\n",
       "6  60007       0       0       0  ...         0         0         0         0\n",
       "7  60008       0       0       0  ...         0         0         0         0\n",
       "8  60009       0       0       0  ...         0         0         0         0\n",
       "9  60010       0       0       0  ...         0         0         0         0\n",
       "\n",
       "[10 rows x 785 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To show 10 first order of test dataset\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "4Ma4hDCWHNcm",
    "outputId": "5e00f842-0714-43cd-ef5e-7d96ebd46aa2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 785)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print shape of test set\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "pkZqYvpQHNcp",
    "outputId": "70ee1d0e-cc5a-4bcd-eb28-d786ea0663a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 785 entries, id to pixel783\n",
      "dtypes: int64(785)\n",
      "memory usage: 59.9 MB\n"
     ]
    }
   ],
   "source": [
    "# To get Number of columns, Index, Datatype and Memory information\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "l4RpJ9NFHNcr",
    "outputId": "3905a5ea-56c0-435a-afb6-08ff1e6d5f3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "pixel0      0\n",
       "pixel1      0\n",
       "pixel2      0\n",
       "pixel3      0\n",
       "           ..\n",
       "pixel779    0\n",
       "pixel780    0\n",
       "pixel781    0\n",
       "pixel782    0\n",
       "pixel783    0\n",
       "Length: 785, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find and count 'NaN' in datasat\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2y9jwSd8HNcx"
   },
   "source": [
    "#### **3.1.3 Reflection on Data Exploding**\n",
    "\n",
    "In this section, it can be seen that dataset is entirely perfect, which is not have any missing or non-null values. However, some of deep learning algorithms required input data as images format. Hence, the data should be prepared by reshaping shape for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8DXRM-VBHNdO"
   },
   "source": [
    "## **3.2 Prepare the Data for Deep Learning Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYp88j7CHNdS"
   },
   "source": [
    "#### **3.2.1 Preparing Data**\n",
    "\n",
    "Some of the deep learning models required the MNIST dataset as 784 inputs; some required an images format as 28x28 pixels. Therefore, to segment train and test variables, they should be separated to segment the input data into the flattened data and image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYPqX-hAIyB6"
   },
   "source": [
    "##### **3.2.1.1 Input as flatten data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "tjsFwXhOIyB7"
   },
   "outputs": [],
   "source": [
    "# Create a copy of data that does not affect to original trian and test set\n",
    "train_f = train.copy()\n",
    "test_f = test.copy()\n",
    "\n",
    "X_f = train_f.values[:,2:]            # Segment train output variables\n",
    "Y_f = train_f.loc[:,'label'].values   # Segment train input variables\n",
    "X_test_f = test_f.values[:,1:]        # Segment test input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Krt9fI-hIyB8",
    "outputId": "d4d6c4c3-0a44-498b-8008-ebb15a5ebef6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_f.shape)\n",
    "print(X_test_f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4M2axaGCIyB-"
   },
   "source": [
    "##### **3.2.1.2 Input as image data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KK7D8KtHIyB-"
   },
   "outputs": [],
   "source": [
    "# Create a copy of data that does not affect to original trian and test set\n",
    "train_i = train.copy()\n",
    "test_i = test.copy()\n",
    "\n",
    "X_i = train_i.values[:,2:]            # Segment train output variables\n",
    "Y_i = train_i.loc[:,'label'].values   # Segment train input variables\n",
    "X_testing_i = test_i.values[:,1:]        # Segment test input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "tnidOw2vIyCA",
    "outputId": "f6af2c96-4a98-4410-f8c1-65ab7bc794ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_i.shape)\n",
    "print(X_testing_i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jPcc3O03IyCC"
   },
   "outputs": [],
   "source": [
    "X_i = np.array(X_i).reshape(60000,28, 28)\n",
    "X_testing_i = np.array(X_testing_i).reshape(10000,28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "TOeWnhPXIyCD",
    "outputId": "10422af3-d34b-48d2-fcb5-25c2c1f3e4bd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_i.shape)\n",
    "print(X_testing_i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F16FZ-MVHNdP"
   },
   "source": [
    "#### **3.2.2 Spliting the train data and segment test data**\n",
    "\n",
    "According to the obstruction of the testing dataset, it has no target column of the label. Therefore, the training dataset has to be used as both training and testing. However, the evaluation of a model skill only the training dataset would result in a biased score. To avoid prediction bias of the dataset, it has to split into three datasets that are training, validation and testing setâ€”starting from separate both datasets into training data and validation data as 70% and 30% respectively.\n",
    "\n",
    "- The training set is used for learning, that is to fit the parameters of the Classification.\n",
    "- The validation set is used to tune the parameters of the Classification.\n",
    "- The test set is used only to assess the performance of a fully-specified Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "aBeldFfwIyCF"
   },
   "outputs": [],
   "source": [
    "import sklearn   # import tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction\n",
    "from sklearn import model_selection, preprocessing  # import model selection and preprocessing\n",
    "from sklearn.preprocessing import scale  # import data normalise tool\n",
    "from sklearn.model_selection import train_test_split   # to divide dataset into train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IeC60YXeIyCG"
   },
   "source": [
    "##### **3.2.2.1 Spliting for flatten data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QYaT3vIqHNdP"
   },
   "outputs": [],
   "source": [
    "# Spliting train and validation set into 70:30 ratio\n",
    "X_train_f, X_valid_f, Y_train_f, Y_valid_f = sklearn.model_selection.train_test_split(X_f, Y_f, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3vRRfEzYIyCH"
   },
   "source": [
    "##### **3.2.2.2 Spliting for image data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jaSKqxkHIyCI"
   },
   "outputs": [],
   "source": [
    "# Spliting train and validation set into 70:30 ratio\n",
    "X_train_i, X_valid_i, Y_train_i, Y_valid_i = sklearn.model_selection.train_test_split(X_i, Y_i, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3gqQY_nIyCJ"
   },
   "source": [
    "#### **3.2.3 Normalising data**\n",
    "\n",
    "Using the MNIST dataset, the value of this dataset is between 0-255 (single grayscale). Therefore, for using the Gradient Descent, the datasets have to adjust values measured on different scales to a notionally standard scale. If not, this would produce math range errors with the sigmoid function as the data is more massive negative values would be produced on a later layer. This process is known as \"normalisation\". Furthermore, it adjusted a range of the pixel data into [0,1] by dividing by 255.0 before it input into algorithms, and also converts them to floats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hO7HBgpFIyCJ"
   },
   "source": [
    "##### **3.2.3.1 Normalisation flatten data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "04nVNDzPIyCJ"
   },
   "outputs": [],
   "source": [
    "# Normalisation scale of X_train and X_valid\n",
    "X_train_f = X_train_f/255\n",
    "X_valid_f = X_valid_f/255\n",
    "Y_train_f = Y_train_f\n",
    "Y_valid_f = Y_valid_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "vqt92W3dIyCL",
    "outputId": "10f9bb3b-6ca5-47dd-b2bc-7c34b0df8dec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 784\n",
      "Number of test examples = 784\n",
      "X_train_f shape: (42000, 784)\n",
      "Y_train_f shape: (42000,)\n",
      "X_valid_f shape: (18000, 784)\n",
      "Y_valid_f shape: (18000,)\n",
      "X_test_f shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Re-ckeck shape again after Normalisation\n",
    "print (\"Number of training examples = \" + str(X_train_f.shape[1]))\n",
    "print (\"Number of test examples = \" + str(X_test_f.shape[1]))\n",
    "print (\"X_train_f shape: \" + str(X_train_f.shape))\n",
    "print (\"Y_train_f shape: \" + str(Y_train_f.shape))\n",
    "print (\"X_valid_f shape: \" + str(X_valid_f.shape))\n",
    "print (\"Y_valid_f shape: \" + str(Y_valid_f.shape))\n",
    "print (\"X_test_f shape: \" + str(X_test_f.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "ZzvDtpT4IyCM",
    "outputId": "3156c632-a92d-44f4-8d2e-772892b2b44c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_f dtype: float64\n",
      "Y_train_f dtype: int64\n",
      "X_valid_f dtype: float64\n",
      "Y_valid_f dtype: int64\n",
      "X_test_f dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Re-ckeck type again after Normalisation\n",
    "print (\"X_train_f dtype: \" + str(X_train_f.dtype))\n",
    "print (\"Y_train_f dtype: \" + str(Y_train_f.dtype))\n",
    "print (\"X_valid_f dtype: \" + str(X_valid_f.dtype))\n",
    "print (\"Y_valid_f dtype: \" + str(Y_valid_f.dtype))\n",
    "print (\"X_test_f dtype: \" + str(X_test_f.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cLrTVT4cIyCN"
   },
   "source": [
    "##### **3.2.3.2 Reshape and Normalisation image data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "EL8kX577IyCO",
    "outputId": "2af0517b-de25-4edd-854f-df6572c7df1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (42000, 28, 28, 1)\n",
      "42000 train samples\n",
      "18000 validation samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = X_i.shape[1], X_i.shape[1]\n",
    "\n",
    "# Reshape the image \n",
    "X_train_i = X_train_i.reshape(X_train_i.shape[0], img_rows, img_cols, 1)\n",
    "X_valid_i = X_valid_i.reshape(X_valid_i.shape[0], img_rows, img_cols, 1)\n",
    "X_test_i = X_testing_i.reshape(X_testing_i.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)  \n",
    "\n",
    "X_train_i = X_train_i.astype('float32')\n",
    "X_valid_i = X_valid_i.astype('float32')\n",
    "X_test_i = X_test_i.astype('float32')\n",
    "\n",
    "# Normalisation scale of X_train and X_valid\n",
    "X_train_i = X_train_i/255\n",
    "X_valid_i = X_valid_i/255\n",
    "X_test_i = X_test_i/255\n",
    "Y_train_i = Y_train_i\n",
    "Y_valid_i = Y_valid_i\n",
    "print('x_train shape:', X_train_i.shape)\n",
    "print(X_train_i.shape[0], 'train samples')\n",
    "print(X_valid_i.shape[0], 'validation samples')\n",
    "print(X_test_i.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "Y_train_i = keras.utils.to_categorical(Y_train_i, num_classes)\n",
    "Y_valid_i = keras.utils.to_categorical(Y_valid_i, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "3N8CA034IyCP",
    "outputId": "69d5cb95-8332-4468-ee90-805a7cf4ab46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = (28, 28)\n",
      "Number of test examples = (28, 28)\n",
      "X_train_i shape: (42000, 28, 28, 1)\n",
      "Y_train_i shape: (42000, 10)\n",
      "X_valid_i shape: (18000, 28, 28, 1)\n",
      "Y_valid_i shape: (18000, 10)\n",
      "X_test_i shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Re-ckeck shape again after Normalisation\n",
    "print (\"Number of training examples = \" + str(X_train_i.shape[1:3]))\n",
    "print (\"Number of test examples = \" + str(X_test_i.shape[1:3]))\n",
    "print (\"X_train_i shape: \" + str(X_train_i.shape))\n",
    "print (\"Y_train_i shape: \" + str(Y_train_i.shape))\n",
    "print (\"X_valid_i shape: \" + str(X_valid_i.shape))\n",
    "print (\"Y_valid_i shape: \" + str(Y_valid_i.shape))\n",
    "print (\"X_test_i shape: \" + str(X_test_i.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "D1QxEFt1IyCQ",
    "outputId": "477cda65-1f7a-4ca9-94f7-8152bd63bcba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_i dtype: float32\n",
      "Y_train_i dtype: float32\n",
      "X_valid_i dtype: float32\n",
      "Y_valid_i dtype: float32\n",
      "X_test_i dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Re-ckeck type again after Normalisation\n",
    "print (\"X_train_i dtype: \" + str(X_train_i.dtype))\n",
    "print (\"Y_train_i dtype: \" + str(Y_train_i.dtype))\n",
    "print (\"X_valid_i dtype: \" + str(X_valid_i.dtype))\n",
    "print (\"Y_valid_i dtype: \" + str(Y_valid_i.dtype))\n",
    "print (\"X_test_i dtype: \" + str(X_test_i.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iC7sXQUN1Nbq"
   },
   "source": [
    "#### **3.2.4 Showing image data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "g-M3n4wlPYg5"
   },
   "outputs": [],
   "source": [
    "# Python 2D plotting library and it's numerical mathematics extension NumPy\n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "POazwxia1MO5",
    "outputId": "3bd19f01-5e6a-4af8-c82c-ba4bcb7de0fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATsAAAD7CAYAAAAVQzPHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWLgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X_i.shape)\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_i[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_i[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_i[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_i[3], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "B3Ep_4MsLL_d",
    "outputId": "48387a4a-6fa0-47bf-a1f4-39f3c8aa31bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATsAAAD7CAYAAAAVQzPHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt   # Python 2D plotting library and it's numerical mathematics extension NumPy\n",
    "\n",
    "print(X_testing_i.shape)\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_testing_i[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_testing_i[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_testing_i[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_testing_i[3], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1aPSUe0fIyCR"
   },
   "source": [
    "## **3.3 Deep Learning Analysis Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTL50me91hzo"
   },
   "source": [
    "#### **3.3.1 Creating a Fully Connected Neural Network model**\n",
    "\n",
    "Firstly, we will start with a Fully Connected Neural Network model, which is the beginning choice to start implementing. Traditionally, the neural networks have three types of layer, which are input layer, hidden layer and output layer. For input layer, Fully Connected Neural Network have to input as a Flatten layer so that the input shape should be all columns in training dataset. Next layer is hidden layer. However, first thing to deal with these dense layers is to consider structure of each factor. There are two decisions, which are how many hidden layers actually to have in the neural networks and how many neurons would be in each of these layers.\n",
    "\n",
    "According to this, using too few neurons would result in the underfitting result. In the other hand, using too many neurons could result in an overfitting result or waste of time-consuming. Hence, before starting model, Heaton (2015) said that there are some rule-of-thumb methods for determining an adequate number of neurons in the hidden layers, which is better than trial and error from throwing a random number at the start modelling as the following:\n",
    "\n",
    "- The number of the hidden neurons should be between the size of input layer and the size of output layer.\n",
    "- The number of the hidden neurons should be two-thirds of the size of input layer, plus the size of output layer.\n",
    "- The number of the hidden neurons should be less than twice the size of input layer.\n",
    "\n",
    "\n",
    "In this project, the last rule-of-thumb methods are chosen, which provide less number of neurons and less running-time. Thus, the first hidden layer is set as 392 neurons. Furthermore, other hidden layers normally are added as half of the first hidden layer, which is 196 neurons until the last hidden layer.\n",
    "\n",
    "Moreover, using the \"Relu\" activation function to all of the hidden layers to avoid the Vanishing Gradients problem and to improve the flow of gradients through the model as well. The \"softmax\" activation function is a frequently appended to the last layer of an image classification, which can interpret the numeric output of the last hidden layer of multi-class classification into probabilities and guarantee that the output probabilities would be in a range of 0 and 1 and all sum up to 1 by taking the delegates of each output. Then, it will normalise each number by summing exponents so that all probabilities would be added up to one. Furthermore, this function could predict the class with the highest estimated probability and gives some perspective, which is suitable for interpreting probabilities for the categories.\n",
    "\n",
    "For compiling model, the loss functions and optimisers should also be considered. Referring to the project target, it is not a binary target so that using the sparse_categorical_crossentropy loss is better for integer targets. Moreover, the optimiser is the main approach used for training a model to minimise the error rate. There are two types of optimiser for making a decision. The first one is Stochastic Gradient Descent (SGD), which is the most common method used to optimise deep learning networks. Another one is the Adaptive Moment Estimation (Adam). It has a dynamic bound of the learning rate, which illustrates a faster convergence and better generalisation than SGD optimise. However, Adam does not always give better training performance in every solution. Therefore, this project would produce both optimisers in the difference epochs, then analyse the result in the further term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZqLJn01IyCT"
   },
   "source": [
    "##### **3.3.1.1 Using Stochastic Gradient Descent (SGD) optimiser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ydbNMyE7IyCT"
   },
   "outputs": [],
   "source": [
    "# Create a copy of data that does not affect to original trian and test set\n",
    "X_train = X_train_f.copy()\n",
    "X_valid = X_valid_f.copy()\n",
    "Y_train = Y_train_f.copy()\n",
    "Y_valid = Y_valid_f.copy()\n",
    "X_test = X_test_f.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "cKyB0RhhIyCU"
   },
   "outputs": [],
   "source": [
    "# Define keys factors\n",
    "image_size = X_train.shape[1]\n",
    "num_classes = 10                # Number of classification target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "LiX1UsEqIyC4",
    "outputId": "a303d4fa-e627-4a62-f977-e5a0c12e70f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rsjBBxsGPYhC"
   },
   "outputs": [],
   "source": [
    "# Perform input layer, hidden layer, and out put layer\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=image_size),                               \n",
    "    tf.keras.layers.Dense(units=392, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=196, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=196, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=196, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=196, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=num_classes, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "yYMDytGmIyCW",
    "outputId": "c3ae5e4a-33a2-468c-8319-6d0337ae0df7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 392)               307720    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 196)               77028     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 196)               38612     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 196)               38612     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 196)               38612     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1970      \n",
      "=================================================================\n",
      "Total params: 502,554\n",
      "Trainable params: 502,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Displays all model's layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "bnrl3IPJIyCY",
    "outputId": "6e5586e6-7483-475a-9082-c51196f8c30d",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Dense at 0x7fa21bfd17f0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa21bf3b278>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa21bf3b748>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa21bf3b9b0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa21bf3bc18>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa21bf3be80>]"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a model's list of layers\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "s5__052TIyCZ"
   },
   "outputs": [],
   "source": [
    "# Specify weights & biases of each layers\n",
    "weights_in, biases_in = model.layers[0].get_weights()\n",
    "weights_h1, biases_h1 = model.layers[1].get_weights()\n",
    "weights_h2, biases_h2 = model.layers[2].get_weights()\n",
    "weights_h3, biases_h3 = model.layers[3].get_weights()\n",
    "weights_h4, biases_h4 = model.layers[4].get_weights()\n",
    "weights_h5, biases_h5 = model.layers[5].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5k1pPFOiIyCa"
   },
   "outputs": [],
   "source": [
    "# Compiling the model by using SGD\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam,Adadelta,SGD\n",
    "\n",
    "optimizer = SGD(learning_rate=0.01, momentum=0.0, nesterov=False)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GCtPK5jIyCc"
   },
   "source": [
    "##### ***Training the Model***\n",
    "\n",
    "The numbers of epochs are related to how diverse the dataset. As the number of epochs increases, more number of times the weight is changed, the curve could be interpreted as underfitting to optimal to overfitting curve. However, we will set high epochs of SGD at 160 as initial epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "pCTeNI5lIyCc",
    "outputId": "a048b2b8-5f88-4a86-b5d4-cefc84f68531",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.9264 - accuracy: 0.7342 - val_loss: 0.2657 - val_accuracy: 0.9227\n",
      "Epoch 2/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.2317 - accuracy: 0.9324 - val_loss: 0.2188 - val_accuracy: 0.9360\n",
      "Epoch 3/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.1821 - accuracy: 0.9483 - val_loss: 0.1796 - val_accuracy: 0.9493\n",
      "Epoch 4/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.1516 - accuracy: 0.9580 - val_loss: 0.1727 - val_accuracy: 0.9506\n",
      "Epoch 5/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.1302 - accuracy: 0.9629 - val_loss: 0.1605 - val_accuracy: 0.9547\n",
      "Epoch 6/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.1109 - accuracy: 0.9688 - val_loss: 0.1497 - val_accuracy: 0.9561\n",
      "Epoch 7/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0949 - accuracy: 0.9741 - val_loss: 0.1416 - val_accuracy: 0.9592\n",
      "Epoch 8/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0819 - accuracy: 0.9770 - val_loss: 0.1367 - val_accuracy: 0.9617\n",
      "Epoch 9/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0704 - accuracy: 0.9808 - val_loss: 0.1267 - val_accuracy: 0.9636\n",
      "Epoch 10/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0599 - accuracy: 0.9838 - val_loss: 0.1266 - val_accuracy: 0.9642\n",
      "Epoch 11/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0523 - accuracy: 0.9859 - val_loss: 0.1237 - val_accuracy: 0.9658\n",
      "Epoch 12/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0443 - accuracy: 0.9882 - val_loss: 0.1191 - val_accuracy: 0.9676\n",
      "Epoch 13/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0370 - accuracy: 0.9911 - val_loss: 0.1163 - val_accuracy: 0.9679\n",
      "Epoch 14/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0320 - accuracy: 0.9922 - val_loss: 0.1175 - val_accuracy: 0.9681\n",
      "Epoch 15/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0271 - accuracy: 0.9932 - val_loss: 0.1214 - val_accuracy: 0.9678\n",
      "Epoch 16/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0220 - accuracy: 0.9949 - val_loss: 0.1225 - val_accuracy: 0.9674\n",
      "Epoch 17/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0189 - accuracy: 0.9959 - val_loss: 0.1224 - val_accuracy: 0.9697\n",
      "Epoch 18/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0153 - accuracy: 0.9970 - val_loss: 0.1230 - val_accuracy: 0.9694\n",
      "Epoch 19/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0127 - accuracy: 0.9978 - val_loss: 0.1577 - val_accuracy: 0.9636\n",
      "Epoch 20/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0107 - accuracy: 0.9980 - val_loss: 0.1281 - val_accuracy: 0.9698\n",
      "Epoch 21/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0082 - accuracy: 0.9987 - val_loss: 0.1338 - val_accuracy: 0.9692\n",
      "Epoch 22/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0067 - accuracy: 0.9990 - val_loss: 0.1370 - val_accuracy: 0.9698\n",
      "Epoch 23/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0056 - accuracy: 0.9992 - val_loss: 0.1366 - val_accuracy: 0.9707\n",
      "Epoch 24/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0045 - accuracy: 0.9995 - val_loss: 0.1400 - val_accuracy: 0.9699\n",
      "Epoch 25/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0038 - accuracy: 0.9997 - val_loss: 0.1400 - val_accuracy: 0.9707\n",
      "Epoch 26/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0029 - accuracy: 0.9998 - val_loss: 0.1438 - val_accuracy: 0.9695\n",
      "Epoch 27/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0025 - accuracy: 0.9999 - val_loss: 0.1460 - val_accuracy: 0.9704\n",
      "Epoch 28/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 0.1476 - val_accuracy: 0.9698\n",
      "Epoch 29/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1504 - val_accuracy: 0.9698\n",
      "Epoch 30/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1508 - val_accuracy: 0.9697\n",
      "Epoch 31/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1524 - val_accuracy: 0.9697\n",
      "Epoch 32/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1538 - val_accuracy: 0.9701\n",
      "Epoch 33/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1567 - val_accuracy: 0.9696\n",
      "Epoch 34/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1578 - val_accuracy: 0.9701\n",
      "Epoch 35/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 9.6583e-04 - accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9701\n",
      "Epoch 36/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 8.9661e-04 - accuracy: 1.0000 - val_loss: 0.1600 - val_accuracy: 0.9700\n",
      "Epoch 37/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 8.3302e-04 - accuracy: 1.0000 - val_loss: 0.1614 - val_accuracy: 0.9700\n",
      "Epoch 38/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 7.6777e-04 - accuracy: 1.0000 - val_loss: 0.1624 - val_accuracy: 0.9696\n",
      "Epoch 39/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 7.2550e-04 - accuracy: 1.0000 - val_loss: 0.1636 - val_accuracy: 0.9699\n",
      "Epoch 40/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.7416e-04 - accuracy: 1.0000 - val_loss: 0.1645 - val_accuracy: 0.9699\n",
      "Epoch 41/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.3640e-04 - accuracy: 1.0000 - val_loss: 0.1657 - val_accuracy: 0.9698\n",
      "Epoch 42/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.0351e-04 - accuracy: 1.0000 - val_loss: 0.1669 - val_accuracy: 0.9698\n",
      "Epoch 43/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 5.7021e-04 - accuracy: 1.0000 - val_loss: 0.1675 - val_accuracy: 0.9701\n",
      "Epoch 44/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 5.3900e-04 - accuracy: 1.0000 - val_loss: 0.1684 - val_accuracy: 0.9699\n",
      "Epoch 45/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 5.1196e-04 - accuracy: 1.0000 - val_loss: 0.1691 - val_accuracy: 0.9699\n",
      "Epoch 46/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 4.8942e-04 - accuracy: 1.0000 - val_loss: 0.1697 - val_accuracy: 0.9698\n",
      "Epoch 47/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 4.6755e-04 - accuracy: 1.0000 - val_loss: 0.1714 - val_accuracy: 0.9700\n",
      "Epoch 48/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 4.4569e-04 - accuracy: 1.0000 - val_loss: 0.1723 - val_accuracy: 0.9698\n",
      "Epoch 49/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 4.2475e-04 - accuracy: 1.0000 - val_loss: 0.1726 - val_accuracy: 0.9695\n",
      "Epoch 50/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 4.0966e-04 - accuracy: 1.0000 - val_loss: 0.1736 - val_accuracy: 0.9698\n",
      "Epoch 51/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 3.9191e-04 - accuracy: 1.0000 - val_loss: 0.1742 - val_accuracy: 0.9698\n",
      "Epoch 52/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 3.7701e-04 - accuracy: 1.0000 - val_loss: 0.1746 - val_accuracy: 0.9697\n",
      "Epoch 53/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 3.6195e-04 - accuracy: 1.0000 - val_loss: 0.1754 - val_accuracy: 0.9698\n",
      "Epoch 54/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 3.4933e-04 - accuracy: 1.0000 - val_loss: 0.1761 - val_accuracy: 0.9696\n",
      "Epoch 55/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 3.3675e-04 - accuracy: 1.0000 - val_loss: 0.1771 - val_accuracy: 0.9700\n",
      "Epoch 56/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 3.2607e-04 - accuracy: 1.0000 - val_loss: 0.1771 - val_accuracy: 0.9698\n",
      "Epoch 57/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 3.1412e-04 - accuracy: 1.0000 - val_loss: 0.1778 - val_accuracy: 0.9698\n",
      "Epoch 58/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 3.0366e-04 - accuracy: 1.0000 - val_loss: 0.1792 - val_accuracy: 0.9701\n",
      "Epoch 59/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 2.9446e-04 - accuracy: 1.0000 - val_loss: 0.1799 - val_accuracy: 0.9697\n",
      "Epoch 60/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 2.8516e-04 - accuracy: 1.0000 - val_loss: 0.1798 - val_accuracy: 0.9696\n",
      "Epoch 61/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 2.7577e-04 - accuracy: 1.0000 - val_loss: 0.1803 - val_accuracy: 0.9700\n",
      "Epoch 62/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 2.6839e-04 - accuracy: 1.0000 - val_loss: 0.1814 - val_accuracy: 0.9697\n",
      "Epoch 63/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 2.6036e-04 - accuracy: 1.0000 - val_loss: 0.1813 - val_accuracy: 0.9698\n",
      "Epoch 64/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 2.5296e-04 - accuracy: 1.0000 - val_loss: 0.1818 - val_accuracy: 0.9699\n",
      "Epoch 65/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 2.4595e-04 - accuracy: 1.0000 - val_loss: 0.1825 - val_accuracy: 0.9697\n",
      "Epoch 66/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 2.3947e-04 - accuracy: 1.0000 - val_loss: 0.1831 - val_accuracy: 0.9697\n",
      "Epoch 67/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 2.3229e-04 - accuracy: 1.0000 - val_loss: 0.1835 - val_accuracy: 0.9698\n",
      "Epoch 68/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 2.2602e-04 - accuracy: 1.0000 - val_loss: 0.1839 - val_accuracy: 0.9698\n",
      "Epoch 69/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 2.2067e-04 - accuracy: 1.0000 - val_loss: 0.1840 - val_accuracy: 0.9699\n",
      "Epoch 70/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 2.1491e-04 - accuracy: 1.0000 - val_loss: 0.1849 - val_accuracy: 0.9701\n",
      "Epoch 71/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 2.0983e-04 - accuracy: 1.0000 - val_loss: 0.1854 - val_accuracy: 0.9700\n",
      "Epoch 72/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 2.0507e-04 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9698\n",
      "Epoch 73/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.9943e-04 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 0.9699\n",
      "Epoch 74/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.9501e-04 - accuracy: 1.0000 - val_loss: 0.1865 - val_accuracy: 0.9697\n",
      "Epoch 75/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.9041e-04 - accuracy: 1.0000 - val_loss: 0.1870 - val_accuracy: 0.9699\n",
      "Epoch 76/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.8686e-04 - accuracy: 1.0000 - val_loss: 0.1874 - val_accuracy: 0.9697\n",
      "Epoch 77/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.8214e-04 - accuracy: 1.0000 - val_loss: 0.1877 - val_accuracy: 0.9701\n",
      "Epoch 78/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.7845e-04 - accuracy: 1.0000 - val_loss: 0.1882 - val_accuracy: 0.9698\n",
      "Epoch 79/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.7414e-04 - accuracy: 1.0000 - val_loss: 0.1886 - val_accuracy: 0.9697\n",
      "Epoch 80/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.7111e-04 - accuracy: 1.0000 - val_loss: 0.1890 - val_accuracy: 0.9698\n",
      "Epoch 81/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.6756e-04 - accuracy: 1.0000 - val_loss: 0.1894 - val_accuracy: 0.9697\n",
      "Epoch 82/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.6391e-04 - accuracy: 1.0000 - val_loss: 0.1899 - val_accuracy: 0.9698\n",
      "Epoch 83/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.6066e-04 - accuracy: 1.0000 - val_loss: 0.1906 - val_accuracy: 0.9696\n",
      "Epoch 84/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.5769e-04 - accuracy: 1.0000 - val_loss: 0.1903 - val_accuracy: 0.9699\n",
      "Epoch 85/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.5443e-04 - accuracy: 1.0000 - val_loss: 0.1909 - val_accuracy: 0.9698\n",
      "Epoch 86/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.5158e-04 - accuracy: 1.0000 - val_loss: 0.1914 - val_accuracy: 0.9698\n",
      "Epoch 87/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.4859e-04 - accuracy: 1.0000 - val_loss: 0.1918 - val_accuracy: 0.9699\n",
      "Epoch 88/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.4606e-04 - accuracy: 1.0000 - val_loss: 0.1921 - val_accuracy: 0.9696\n",
      "Epoch 89/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.4336e-04 - accuracy: 1.0000 - val_loss: 0.1924 - val_accuracy: 0.9698\n",
      "Epoch 90/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.4069e-04 - accuracy: 1.0000 - val_loss: 0.1927 - val_accuracy: 0.9699\n",
      "Epoch 91/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.3815e-04 - accuracy: 1.0000 - val_loss: 0.1929 - val_accuracy: 0.9699\n",
      "Epoch 92/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.3561e-04 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9698\n",
      "Epoch 93/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.3319e-04 - accuracy: 1.0000 - val_loss: 0.1935 - val_accuracy: 0.9698\n",
      "Epoch 94/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.3119e-04 - accuracy: 1.0000 - val_loss: 0.1940 - val_accuracy: 0.9698\n",
      "Epoch 95/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.2882e-04 - accuracy: 1.0000 - val_loss: 0.1944 - val_accuracy: 0.9697\n",
      "Epoch 96/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.2667e-04 - accuracy: 1.0000 - val_loss: 0.1947 - val_accuracy: 0.9697\n",
      "Epoch 97/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.2454e-04 - accuracy: 1.0000 - val_loss: 0.1950 - val_accuracy: 0.9698\n",
      "Epoch 98/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.2268e-04 - accuracy: 1.0000 - val_loss: 0.1953 - val_accuracy: 0.9699\n",
      "Epoch 99/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.2052e-04 - accuracy: 1.0000 - val_loss: 0.1955 - val_accuracy: 0.9699\n",
      "Epoch 100/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.1869e-04 - accuracy: 1.0000 - val_loss: 0.1959 - val_accuracy: 0.9699\n",
      "Epoch 101/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.1684e-04 - accuracy: 1.0000 - val_loss: 0.1960 - val_accuracy: 0.9700\n",
      "Epoch 102/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.1497e-04 - accuracy: 1.0000 - val_loss: 0.1966 - val_accuracy: 0.9696\n",
      "Epoch 103/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.1311e-04 - accuracy: 1.0000 - val_loss: 0.1968 - val_accuracy: 0.9698\n",
      "Epoch 104/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.1154e-04 - accuracy: 1.0000 - val_loss: 0.1971 - val_accuracy: 0.9698\n",
      "Epoch 105/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.0991e-04 - accuracy: 1.0000 - val_loss: 0.1974 - val_accuracy: 0.9697\n",
      "Epoch 106/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.0843e-04 - accuracy: 1.0000 - val_loss: 0.1977 - val_accuracy: 0.9697\n",
      "Epoch 107/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.0663e-04 - accuracy: 1.0000 - val_loss: 0.1978 - val_accuracy: 0.9698\n",
      "Epoch 108/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.0520e-04 - accuracy: 1.0000 - val_loss: 0.1984 - val_accuracy: 0.9697\n",
      "Epoch 109/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.0379e-04 - accuracy: 1.0000 - val_loss: 0.1984 - val_accuracy: 0.9696\n",
      "Epoch 110/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.0237e-04 - accuracy: 1.0000 - val_loss: 0.1987 - val_accuracy: 0.9697\n",
      "Epoch 111/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 1.0068e-04 - accuracy: 1.0000 - val_loss: 0.1986 - val_accuracy: 0.9699\n",
      "Epoch 112/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 9.9525e-05 - accuracy: 1.0000 - val_loss: 0.1991 - val_accuracy: 0.9698\n",
      "Epoch 113/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 9.7949e-05 - accuracy: 1.0000 - val_loss: 0.1992 - val_accuracy: 0.9699\n",
      "Epoch 114/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 9.6789e-05 - accuracy: 1.0000 - val_loss: 0.1999 - val_accuracy: 0.9698\n",
      "Epoch 115/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 9.5390e-05 - accuracy: 1.0000 - val_loss: 0.2000 - val_accuracy: 0.9698\n",
      "Epoch 116/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 9.4111e-05 - accuracy: 1.0000 - val_loss: 0.2001 - val_accuracy: 0.9698\n",
      "Epoch 117/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 9.3288e-05 - accuracy: 1.0000 - val_loss: 0.2005 - val_accuracy: 0.9698\n",
      "Epoch 118/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 9.1737e-05 - accuracy: 1.0000 - val_loss: 0.2006 - val_accuracy: 0.9699\n",
      "Epoch 119/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 9.0641e-05 - accuracy: 1.0000 - val_loss: 0.2009 - val_accuracy: 0.9697\n",
      "Epoch 120/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 8.9450e-05 - accuracy: 1.0000 - val_loss: 0.2011 - val_accuracy: 0.9699\n",
      "Epoch 121/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 8.8326e-05 - accuracy: 1.0000 - val_loss: 0.2014 - val_accuracy: 0.9697\n",
      "Epoch 122/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 8.7268e-05 - accuracy: 1.0000 - val_loss: 0.2017 - val_accuracy: 0.9697\n",
      "Epoch 123/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 8.6241e-05 - accuracy: 1.0000 - val_loss: 0.2017 - val_accuracy: 0.9699\n",
      "Epoch 124/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 8.5249e-05 - accuracy: 1.0000 - val_loss: 0.2019 - val_accuracy: 0.9699\n",
      "Epoch 125/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 8.4043e-05 - accuracy: 1.0000 - val_loss: 0.2021 - val_accuracy: 0.9701\n",
      "Epoch 126/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 8.3008e-05 - accuracy: 1.0000 - val_loss: 0.2027 - val_accuracy: 0.9697\n",
      "Epoch 127/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 8.2045e-05 - accuracy: 1.0000 - val_loss: 0.2028 - val_accuracy: 0.9697\n",
      "Epoch 128/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 8.1219e-05 - accuracy: 1.0000 - val_loss: 0.2029 - val_accuracy: 0.9699\n",
      "Epoch 129/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 8.0332e-05 - accuracy: 1.0000 - val_loss: 0.2032 - val_accuracy: 0.9697\n",
      "Epoch 130/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 7.9275e-05 - accuracy: 1.0000 - val_loss: 0.2035 - val_accuracy: 0.9698\n",
      "Epoch 131/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 7.8663e-05 - accuracy: 1.0000 - val_loss: 0.2035 - val_accuracy: 0.9699\n",
      "Epoch 132/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 7.7630e-05 - accuracy: 1.0000 - val_loss: 0.2037 - val_accuracy: 0.9699\n",
      "Epoch 133/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 7.6573e-05 - accuracy: 1.0000 - val_loss: 0.2041 - val_accuracy: 0.9701\n",
      "Epoch 134/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 7.5774e-05 - accuracy: 1.0000 - val_loss: 0.2041 - val_accuracy: 0.9699\n",
      "Epoch 135/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 7.4951e-05 - accuracy: 1.0000 - val_loss: 0.2044 - val_accuracy: 0.9700\n",
      "Epoch 136/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 7.4017e-05 - accuracy: 1.0000 - val_loss: 0.2049 - val_accuracy: 0.9699\n",
      "Epoch 137/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 7.3394e-05 - accuracy: 1.0000 - val_loss: 0.2048 - val_accuracy: 0.9699\n",
      "Epoch 138/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 7.2432e-05 - accuracy: 1.0000 - val_loss: 0.2050 - val_accuracy: 0.9700\n",
      "Epoch 139/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 7.1787e-05 - accuracy: 1.0000 - val_loss: 0.2052 - val_accuracy: 0.9699\n",
      "Epoch 140/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 7.0986e-05 - accuracy: 1.0000 - val_loss: 0.2055 - val_accuracy: 0.9700\n",
      "Epoch 141/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 7.0231e-05 - accuracy: 1.0000 - val_loss: 0.2058 - val_accuracy: 0.9699\n",
      "Epoch 142/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.9451e-05 - accuracy: 1.0000 - val_loss: 0.2058 - val_accuracy: 0.9699\n",
      "Epoch 143/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.8828e-05 - accuracy: 1.0000 - val_loss: 0.2061 - val_accuracy: 0.9698\n",
      "Epoch 144/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.8101e-05 - accuracy: 1.0000 - val_loss: 0.2066 - val_accuracy: 0.9699\n",
      "Epoch 145/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.7477e-05 - accuracy: 1.0000 - val_loss: 0.2064 - val_accuracy: 0.9699\n",
      "Epoch 146/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.6712e-05 - accuracy: 1.0000 - val_loss: 0.2068 - val_accuracy: 0.9698\n",
      "Epoch 147/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.6151e-05 - accuracy: 1.0000 - val_loss: 0.2068 - val_accuracy: 0.9699\n",
      "Epoch 148/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.5476e-05 - accuracy: 1.0000 - val_loss: 0.2068 - val_accuracy: 0.9699\n",
      "Epoch 149/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.4772e-05 - accuracy: 1.0000 - val_loss: 0.2070 - val_accuracy: 0.9701\n",
      "Epoch 150/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.4229e-05 - accuracy: 1.0000 - val_loss: 0.2072 - val_accuracy: 0.9701\n",
      "Epoch 151/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.3528e-05 - accuracy: 1.0000 - val_loss: 0.2077 - val_accuracy: 0.9701\n",
      "Epoch 152/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.2925e-05 - accuracy: 1.0000 - val_loss: 0.2076 - val_accuracy: 0.9699\n",
      "Epoch 153/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.2382e-05 - accuracy: 1.0000 - val_loss: 0.2079 - val_accuracy: 0.9699\n",
      "Epoch 154/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.1851e-05 - accuracy: 1.0000 - val_loss: 0.2080 - val_accuracy: 0.9699\n",
      "Epoch 155/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.1215e-05 - accuracy: 1.0000 - val_loss: 0.2083 - val_accuracy: 0.9699\n",
      "Epoch 156/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.0614e-05 - accuracy: 1.0000 - val_loss: 0.2085 - val_accuracy: 0.9700\n",
      "Epoch 157/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 6.0092e-05 - accuracy: 1.0000 - val_loss: 0.2085 - val_accuracy: 0.9701\n",
      "Epoch 158/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 5.9613e-05 - accuracy: 1.0000 - val_loss: 0.2088 - val_accuracy: 0.9698\n",
      "Epoch 159/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 5.8958e-05 - accuracy: 1.0000 - val_loss: 0.2090 - val_accuracy: 0.9699\n",
      "Epoch 160/160\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 5.8480e-05 - accuracy: 1.0000 - val_loss: 0.2092 - val_accuracy: 0.9699\n"
     ]
    }
   ],
   "source": [
    "# Training model of validation\n",
    "history = model.fit(X_train, Y_train, epochs=160, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okqyY8m4IyCd"
   },
   "source": [
    "##### ***Evaluating the Model***\n",
    "\n",
    "From the learning curve graph, it provides the comparison between loss and accuracy of training and validation set by changing the model accuracy and number of epochs. It could be seen that both training and validation accuracy are increasing while the loss of both sets are decreasing. However, the validation loss curve is close to the training curve only before approximately 5 epochs, which means the number of epochs more than 5 is overfitting. Moreover, it could be interpreted that the model would not end up the better accuracy on higher epochs. For this model, the best accuracy provide 96.99% with 0.2092 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "LycxNy3xIyCe",
    "outputId": "2b7f3430-3d26-42c0-d58f-d44155ddcfd5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAGFCAYAAADU/MRFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wd0/EUNjVGSHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Learning curve\n",
    "pd.DataFrame(history.history).plot(figsize=(12, 6))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title('Learning Curve', fontsize=15)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "eB7pzROKIyCf",
    "outputId": "69411710-e7d0-4418-d8c9-9657b0a6a873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 1s 2ms/step - loss: 0.2092 - accuracy: 0.9699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2091594934463501, 0.9698888659477234]"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7-mfY5-IyCh"
   },
   "source": [
    "##### ***Making prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "uSIrsBRVIyCh",
    "outputId": "60120070-be24-4112-d2fd-c3cea4e66973",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-45-7a7abbd4210c>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 9])"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test = model.predict_classes(X_test)\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFQ3mAE_IyCi"
   },
   "source": [
    "##### **3.3.1.2 Using Adaptive Moment Estimation (Adam)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "syF58cbhPYhN"
   },
   "outputs": [],
   "source": [
    "# Create a copy of data that does not affect to original trian and test set\n",
    "X_train = X_train_f.copy()\n",
    "X_valid = X_valid_f.copy()\n",
    "Y_train = Y_train_f.copy()\n",
    "Y_valid = Y_valid_f.copy()\n",
    "X_test = X_test_f.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rtn0OzZSIyCj"
   },
   "outputs": [],
   "source": [
    "# Define keys factors\n",
    "image_size = X_train.shape[1]\n",
    "num_classes = 10                # Number of classification target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "OqYhvBfzIyCk"
   },
   "outputs": [],
   "source": [
    "# Perform input layer, hidden layer, and out put layer\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=image_size),                               \n",
    "    tf.keras.layers.Dense(units=392, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=196, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=196, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=196, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=196, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=num_classes, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "VMGpnnhqIyCl",
    "outputId": "a0327647-b436-4ca8-8188-b61e897bff59",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 392)               307720    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 196)               77028     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 196)               38612     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 196)               38612     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 196)               38612     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1970      \n",
      "=================================================================\n",
      "Total params: 502,554\n",
      "Trainable params: 502,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Displays all model's layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "wrZmq7FQIyCm",
    "outputId": "d62163a7-c365-4576-a76e-e15ac62a80fc",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Dense at 0x7fa1f61af4a8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa21c7c3908>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa1f61afb00>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa21c2bb0b8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa1f61aff98>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa1f6386198>]"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a model's list of layers\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fQNJgzeFIyCn"
   },
   "outputs": [],
   "source": [
    "# Specify weights & biases of each layers\n",
    "weights_in, biases_in = model.layers[0].get_weights()\n",
    "weights_h1, biases_h1 = model.layers[1].get_weights()\n",
    "weights_h2, biases_h2 = model.layers[2].get_weights()\n",
    "weights_h3, biases_h3 = model.layers[3].get_weights()\n",
    "weights_h4, biases_h4 = model.layers[4].get_weights()\n",
    "weights_h5, biases_h5 = model.layers[5].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "id7dbEkpIyCp"
   },
   "outputs": [],
   "source": [
    "# Compiling the model by using Adam\n",
    "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geDaDFVdIyCp"
   },
   "source": [
    "##### ***Training the Model***\n",
    "\n",
    "In this section, the epochs of Adam would be used less than SGD, which is 40 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "GGTOgzJ-IyCq",
    "outputId": "b39472c1-b065-4c6d-b25d-884f24b6f355",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.2216 - accuracy: 0.9349 - val_loss: 0.1520 - val_accuracy: 0.9573\n",
      "Epoch 2/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0958 - accuracy: 0.9729 - val_loss: 0.1185 - val_accuracy: 0.9666\n",
      "Epoch 3/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0697 - accuracy: 0.9800 - val_loss: 0.1319 - val_accuracy: 0.9640\n",
      "Epoch 4/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0520 - accuracy: 0.9850 - val_loss: 0.1147 - val_accuracy: 0.9723\n",
      "Epoch 5/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0413 - accuracy: 0.9878 - val_loss: 0.1059 - val_accuracy: 0.9760\n",
      "Epoch 6/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0338 - accuracy: 0.9902 - val_loss: 0.1199 - val_accuracy: 0.9747\n",
      "Epoch 7/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0281 - accuracy: 0.9925 - val_loss: 0.1198 - val_accuracy: 0.9693\n",
      "Epoch 8/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0264 - accuracy: 0.9931 - val_loss: 0.1155 - val_accuracy: 0.9769\n",
      "Epoch 9/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0227 - accuracy: 0.9941 - val_loss: 0.1235 - val_accuracy: 0.9764\n",
      "Epoch 10/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0202 - accuracy: 0.9947 - val_loss: 0.1288 - val_accuracy: 0.9772\n",
      "Epoch 11/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0133 - accuracy: 0.9962 - val_loss: 0.1365 - val_accuracy: 0.9769\n",
      "Epoch 12/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.1364 - val_accuracy: 0.9768\n",
      "Epoch 13/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0171 - accuracy: 0.9956 - val_loss: 0.1332 - val_accuracy: 0.9760\n",
      "Epoch 14/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0144 - accuracy: 0.9961 - val_loss: 0.1737 - val_accuracy: 0.9740\n",
      "Epoch 15/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0125 - accuracy: 0.9970 - val_loss: 0.2066 - val_accuracy: 0.9699\n",
      "Epoch 16/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0108 - accuracy: 0.9971 - val_loss: 0.1750 - val_accuracy: 0.9792\n",
      "Epoch 17/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0128 - accuracy: 0.9969 - val_loss: 0.2057 - val_accuracy: 0.9701\n",
      "Epoch 18/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0121 - accuracy: 0.9970 - val_loss: 0.1560 - val_accuracy: 0.9773\n",
      "Epoch 19/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 0.3103 - val_accuracy: 0.9646\n",
      "Epoch 20/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0159 - accuracy: 0.9972 - val_loss: 0.1502 - val_accuracy: 0.9789\n",
      "Epoch 21/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0096 - accuracy: 0.9977 - val_loss: 0.1723 - val_accuracy: 0.9786\n",
      "Epoch 22/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0087 - accuracy: 0.9984 - val_loss: 0.1910 - val_accuracy: 0.9777\n",
      "Epoch 23/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0113 - accuracy: 0.9976 - val_loss: 0.2074 - val_accuracy: 0.9780\n",
      "Epoch 24/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0102 - accuracy: 0.9980 - val_loss: 0.1816 - val_accuracy: 0.9788\n",
      "Epoch 25/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0084 - accuracy: 0.9983 - val_loss: 0.2630 - val_accuracy: 0.9715\n",
      "Epoch 26/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0129 - accuracy: 0.9979 - val_loss: 0.1995 - val_accuracy: 0.9770\n",
      "Epoch 27/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0085 - accuracy: 0.9986 - val_loss: 0.1822 - val_accuracy: 0.9789\n",
      "Epoch 28/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0071 - accuracy: 0.9987 - val_loss: 0.1806 - val_accuracy: 0.9792\n",
      "Epoch 29/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 0.2504 - val_accuracy: 0.9781\n",
      "Epoch 30/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0086 - accuracy: 0.9982 - val_loss: 0.2713 - val_accuracy: 0.9770\n",
      "Epoch 31/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0112 - accuracy: 0.9982 - val_loss: 0.2201 - val_accuracy: 0.9781\n",
      "Epoch 32/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0114 - accuracy: 0.9981 - val_loss: 0.2469 - val_accuracy: 0.9776\n",
      "Epoch 33/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.2769 - val_accuracy: 0.9778\n",
      "Epoch 34/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.3110 - val_accuracy: 0.9781\n",
      "Epoch 35/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0123 - accuracy: 0.9979 - val_loss: 0.1977 - val_accuracy: 0.9779\n",
      "Epoch 36/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0129 - accuracy: 0.9981 - val_loss: 0.1967 - val_accuracy: 0.9788\n",
      "Epoch 37/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0066 - accuracy: 0.9988 - val_loss: 0.1866 - val_accuracy: 0.9789\n",
      "Epoch 38/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.3025 - val_accuracy: 0.9782\n",
      "Epoch 39/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0106 - accuracy: 0.9987 - val_loss: 0.2801 - val_accuracy: 0.9787\n",
      "Epoch 40/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0098 - accuracy: 0.9983 - val_loss: 0.2733 - val_accuracy: 0.9773\n"
     ]
    }
   ],
   "source": [
    "# Training model of validation\n",
    "history = model.fit(X_train, Y_train, epochs=40, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IPby2q5rIyCr"
   },
   "source": [
    "##### ***Evaluating the Model***\n",
    "\n",
    "From the learning curve graph, It could be seen that the tend of curve is similar to the SGD optimiser. However, it shows a better result, which the best accuracy provide 97.73% with 0.2038 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "HN8-L00PIyCr",
    "outputId": "8eefeb4d-e640-43ae-8744-95aebd05b702"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAGFCAYAAADU/MRFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Learning curve\n",
    "pd.DataFrame(history.history).plot(figsize=(12, 6))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title('Learning Curve', fontsize=15)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "v7pc8XwDIyCs",
    "outputId": "02dde2bf-d6e4-4493-8317-661ffe7b2d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 1s 2ms/step - loss: 0.2733 - accuracy: 0.9773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2732917070388794, 0.9772777557373047]"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "INU5Sb_PIyCt"
   },
   "source": [
    "##### ***Making Prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "6uY4weTwIyCt",
    "outputId": "ba753a3e-d025-442a-ea9f-10dd70c685af",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 9])"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test = model.predict_classes(X_test)\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yr3eb1ZIIyCu"
   },
   "source": [
    "#### **3.3.2 Improving a Fully Connected Neural Network model**\n",
    "\n",
    "Using a model Regularisation is one of the methods to improve the accuracy of the model. The dropout technique will be used, which might boost the accuracy of approximately 1-2% of the model. Referring to the last section, Adam optimiser provides better accuracy than SGD. Therefore, Adam is chosen to run this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KGHZ4vZVIyCu"
   },
   "outputs": [],
   "source": [
    "# Create a copy of data that does not affect to original trian and test set\n",
    "X_train = X_train_f.copy()\n",
    "X_valid = X_valid_f.copy()\n",
    "Y_train = Y_train_f.copy()\n",
    "Y_valid = Y_valid_f.copy()\n",
    "X_test = X_test_f.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yjg0g-YyIyCv"
   },
   "outputs": [],
   "source": [
    "# Define keys factors\n",
    "image_size = X_train.shape[1]\n",
    "num_classes = 10                # Number of classification target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "o2hQgG0VIyCw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform input layer, hidden layer, and out put layer\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=image_size),                               \n",
    "    tf.keras.layers.Dense(units=392, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.3),\n",
    "    tf.keras.layers.Dense(units=196, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.3),\n",
    "    tf.keras.layers.Dense(units=196, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.3),\n",
    "    tf.keras.layers.Dense(units=196, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.3),\n",
    "    tf.keras.layers.Dense(units=196, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.3),\n",
    "    tf.keras.layers.Dense(units=num_classes, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "z1hUi23uIyCx",
    "outputId": "6c127cf8-7c0a-431f-aecd-7710a1ec1b15",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 392)               307720    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 392)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 196)               77028     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 196)               38612     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 196)               38612     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 196)               38612     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                1970      \n",
      "=================================================================\n",
      "Total params: 502,554\n",
      "Trainable params: 502,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Displays all model's layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "eXqE7vuGIyCy",
    "outputId": "7322c14c-861d-4611-b2c8-0f76a8d225ef",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Dense at 0x7fa1daedf390>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7fa1daedf588>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa1daf5ab00>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7fa1daf8f940>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa1daedfb38>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7fa1daedfd30>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa1daedfe48>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7fa1daedfcf8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa1daf7f198>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7fa1daf7f080>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fa1daf7f4a8>]"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a model's list of layers\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "cbw3JSHGIyCz"
   },
   "outputs": [],
   "source": [
    "# Compiling the model by using Adam\n",
    "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n247P-BBIyC0"
   },
   "source": [
    "##### ***Training the Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "hDHc85TbIyC0",
    "outputId": "4ab13ce3-fb0b-47c2-97c1-1af1db1bbfd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.3680 - accuracy: 0.8888 - val_loss: 0.1567 - val_accuracy: 0.9549\n",
      "Epoch 2/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.1630 - accuracy: 0.9558 - val_loss: 0.1241 - val_accuracy: 0.9644\n",
      "Epoch 3/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.1311 - accuracy: 0.9661 - val_loss: 0.1044 - val_accuracy: 0.9709\n",
      "Epoch 4/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.1069 - accuracy: 0.9719 - val_loss: 0.1026 - val_accuracy: 0.9741\n",
      "Epoch 5/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0984 - accuracy: 0.9746 - val_loss: 0.0944 - val_accuracy: 0.9745\n",
      "Epoch 6/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0823 - accuracy: 0.9786 - val_loss: 0.1046 - val_accuracy: 0.9741\n",
      "Epoch 7/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0822 - accuracy: 0.9790 - val_loss: 0.0798 - val_accuracy: 0.9789\n",
      "Epoch 8/40\n",
      "1313/1313 [==============================] - 5s 3ms/step - loss: 0.0676 - accuracy: 0.9824 - val_loss: 0.0905 - val_accuracy: 0.9772\n",
      "Epoch 9/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0675 - accuracy: 0.9828 - val_loss: 0.0998 - val_accuracy: 0.9778\n",
      "Epoch 10/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0597 - accuracy: 0.9846 - val_loss: 0.0951 - val_accuracy: 0.9799\n",
      "Epoch 11/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0579 - accuracy: 0.9848 - val_loss: 0.1077 - val_accuracy: 0.9777\n",
      "Epoch 12/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0528 - accuracy: 0.9861 - val_loss: 0.1089 - val_accuracy: 0.9794\n",
      "Epoch 13/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0494 - accuracy: 0.9873 - val_loss: 0.1072 - val_accuracy: 0.9777\n",
      "Epoch 14/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0506 - accuracy: 0.9877 - val_loss: 0.0996 - val_accuracy: 0.9776\n",
      "Epoch 15/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0509 - accuracy: 0.9878 - val_loss: 0.1060 - val_accuracy: 0.9799\n",
      "Epoch 16/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0441 - accuracy: 0.9892 - val_loss: 0.1160 - val_accuracy: 0.9777\n",
      "Epoch 17/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0422 - accuracy: 0.9899 - val_loss: 0.1002 - val_accuracy: 0.9792\n",
      "Epoch 18/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0405 - accuracy: 0.9899 - val_loss: 0.1058 - val_accuracy: 0.9802\n",
      "Epoch 19/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0384 - accuracy: 0.9909 - val_loss: 0.1279 - val_accuracy: 0.9786\n",
      "Epoch 20/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0395 - accuracy: 0.9903 - val_loss: 0.1418 - val_accuracy: 0.9783\n",
      "Epoch 21/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0419 - accuracy: 0.9907 - val_loss: 0.1141 - val_accuracy: 0.9788\n",
      "Epoch 22/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0403 - accuracy: 0.9907 - val_loss: 0.1112 - val_accuracy: 0.9796\n",
      "Epoch 23/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0392 - accuracy: 0.9915 - val_loss: 0.1152 - val_accuracy: 0.9799\n",
      "Epoch 24/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0323 - accuracy: 0.9929 - val_loss: 0.1312 - val_accuracy: 0.9793\n",
      "Epoch 25/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0356 - accuracy: 0.9917 - val_loss: 0.1101 - val_accuracy: 0.9794\n",
      "Epoch 26/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0319 - accuracy: 0.9923 - val_loss: 0.1380 - val_accuracy: 0.9808\n",
      "Epoch 27/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0312 - accuracy: 0.9925 - val_loss: 0.1411 - val_accuracy: 0.9801\n",
      "Epoch 28/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0339 - accuracy: 0.9923 - val_loss: 0.1458 - val_accuracy: 0.9807\n",
      "Epoch 29/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0305 - accuracy: 0.9932 - val_loss: 0.1591 - val_accuracy: 0.9799\n",
      "Epoch 30/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0300 - accuracy: 0.9936 - val_loss: 0.1380 - val_accuracy: 0.9809\n",
      "Epoch 31/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0263 - accuracy: 0.9937 - val_loss: 0.1707 - val_accuracy: 0.9807\n",
      "Epoch 32/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0332 - accuracy: 0.9926 - val_loss: 0.1354 - val_accuracy: 0.9797\n",
      "Epoch 33/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0320 - accuracy: 0.9936 - val_loss: 0.1387 - val_accuracy: 0.9811\n",
      "Epoch 34/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0246 - accuracy: 0.9950 - val_loss: 0.1778 - val_accuracy: 0.9808\n",
      "Epoch 35/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0322 - accuracy: 0.9939 - val_loss: 0.1643 - val_accuracy: 0.9782\n",
      "Epoch 36/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0329 - accuracy: 0.9937 - val_loss: 0.1402 - val_accuracy: 0.9797\n",
      "Epoch 37/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0271 - accuracy: 0.9941 - val_loss: 0.1666 - val_accuracy: 0.9809\n",
      "Epoch 38/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0268 - accuracy: 0.9944 - val_loss: 0.1560 - val_accuracy: 0.9806\n",
      "Epoch 39/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0322 - accuracy: 0.9941 - val_loss: 0.1444 - val_accuracy: 0.9794\n",
      "Epoch 40/40\n",
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0265 - accuracy: 0.9945 - val_loss: 0.1360 - val_accuracy: 0.9819\n"
     ]
    }
   ],
   "source": [
    "# Training model of validation\n",
    "history = model.fit(X_train, Y_train, epochs=40, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1kL-nkQ-IyC1"
   },
   "source": [
    "##### ***Evaluating the Model***\n",
    "\n",
    "It shows a better result as in theory, which provides an accuracy of 98.19% with 0.1360 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Jw_B9tZrIyC1",
    "outputId": "732d7c43-94b2-4b7a-c780-dffb99c7cbdb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAGFCAYAAADU/MRFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Learning curve\n",
    "pd.DataFrame(history.history).plot(figsize=(12, 6))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title('Learning Curve', fontsize=15)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Rz2Hq_5_IyC2",
    "outputId": "8d36974a-a9b2-4b45-c768-11abcdc11523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 1s 2ms/step - loss: 0.1360 - accuracy: 0.9819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13604997098445892, 0.9818888902664185]"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ffl4QpNfIyC2"
   },
   "source": [
    "##### ***Making prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "cRnYl9LwIyC2",
    "outputId": "2f8f60a2-a14b-40cf-8917-8c2b6da57bbf",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 9])"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test = model.predict_classes(X_test)\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vroCHH7HHNdW"
   },
   "source": [
    "#### **3.3.3 The Convolutional Neural Network (CNN)**\n",
    "Another model is called \"Convolutional Neural Networks\". It is a subset of deep neural network that widely used to analyse imagery. There are many architectures for the CNN approach. However, the excellent design should provide the highest accuracy, while computational complexity was minimised. The advantage of CNN method is parameters of CNN much less than a fully connected network. Thus, the computational complexity can be reduced. In this section would provide the way to predict by using CNN model from the testing set, which is provided in-class.\n",
    "\n",
    "\n",
    "##### ***CNN Technique using LeNet-5 Architecture***\n",
    "LeNet-5 model is one of the most well-known CNN architectures, and it was built by Yann LeCun in 1998. Many researchers used this architecture for hand-written digit recognition (MNIST) for the excellent results. The Kannada (MNIST) is similar to the MNIST, which can use the LeNet-5 to train our model.\n",
    "\n",
    "##### ***Data Preparation for LeNet-5***\n",
    "There are several important details for this architecture. The input of LeNet-5 is 32x32 pixels. Therefore, it can use zero-padding to this dataset to be 32x32 pixels. Padding technique can help to preserve information on the edge of pictures. However, the dataset should be normalised before feeding into the network.\n",
    "\n",
    "[More detail about LeNet-5 architecture can be found here.](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "vLHFNkD7IyC4",
    "outputId": "330f4553-323e-471a-c720-bf11beb886a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Image Shape: (32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Padding images with 0s\n",
    "X_train_LeNet5 = np.pad(X_train_i, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_valid_LeNet5 = np.pad(X_valid_i, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_test_LeNet5 = np.pad(X_test_i, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "\n",
    "Y_train_LeNet5 = Y_train_i\n",
    "Y_valid_LeNet5 = Y_valid_i\n",
    "\n",
    "print(\"Updated Image Shape: {}\".format(X_train_LeNet5[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKrUzYN1PYhl"
   },
   "source": [
    "#### **Hyperparameters Fine-Tuning for CNN Approach**\n",
    "\n",
    "- ***Learning Rate***: One of the most critical hyperparameters is the learning rate. Generally speaking, the average learning rate is about half the actual learning rate.\n",
    "\n",
    "- ***Batch Size***: The batch size will have a significant effect on the efficiency and training time of your model. Furthermore, the optimum batch size would usually be smaller than 32, in April 2018, Yann Lecun said that \"Friends don't let friends use mini-batches greater than 32\".\n",
    "\n",
    "- ***The number of training iterations***: In many cases, the iteration number does not seem necessary because of the early stopping technique. For more details about early stopping can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Ohie5dajIyC5",
    "outputId": "8db38e56-b96d-4d2a-80c3-1bbcad616d01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 6)         60        \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 15, 15, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 16)        880       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 6, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               69240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 81,194\n",
      "Trainable params: 81,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Perform layers\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(32,32,1)),\n",
    "    tf.keras.layers.AveragePooling2D(),\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.AveragePooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=120, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=84, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# Displays all model's layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AKz5t81YhUPz"
   },
   "source": [
    "##### ***Compiling and Fitting the Model***\n",
    "\n",
    "A number of batch size and epochs for CNN will be discussed in the following part. However, we will start using batch size is 128 with 100 epochs to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FDgOo91dIyC6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "N5YWT-foIyC7",
    "outputId": "0fc94a01-ac40-4372-e1b3-7321a0246687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.4229 - accuracy: 0.8880 - val_loss: 0.1688 - val_accuracy: 0.9537\n",
      "Epoch 2/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.1240 - accuracy: 0.9640 - val_loss: 0.1088 - val_accuracy: 0.9702\n",
      "Epoch 3/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0877 - accuracy: 0.9737 - val_loss: 0.0854 - val_accuracy: 0.9758\n",
      "Epoch 4/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0703 - accuracy: 0.9778 - val_loss: 0.0763 - val_accuracy: 0.9769\n",
      "Epoch 5/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0588 - accuracy: 0.9815 - val_loss: 0.0681 - val_accuracy: 0.9804\n",
      "Epoch 6/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0505 - accuracy: 0.9839 - val_loss: 0.0647 - val_accuracy: 0.9803\n",
      "Epoch 7/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0447 - accuracy: 0.9855 - val_loss: 0.0571 - val_accuracy: 0.9827\n",
      "Epoch 8/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0366 - accuracy: 0.9882 - val_loss: 0.0544 - val_accuracy: 0.9833\n",
      "Epoch 9/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0327 - accuracy: 0.9895 - val_loss: 0.0585 - val_accuracy: 0.9835\n",
      "Epoch 10/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0290 - accuracy: 0.9905 - val_loss: 0.0538 - val_accuracy: 0.9837\n",
      "Epoch 11/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0263 - accuracy: 0.9910 - val_loss: 0.0573 - val_accuracy: 0.9838\n",
      "Epoch 12/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0261 - accuracy: 0.9911 - val_loss: 0.0536 - val_accuracy: 0.9848\n",
      "Epoch 13/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0210 - accuracy: 0.9932 - val_loss: 0.0521 - val_accuracy: 0.9857\n",
      "Epoch 14/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0195 - accuracy: 0.9938 - val_loss: 0.0520 - val_accuracy: 0.9862\n",
      "Epoch 15/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0170 - accuracy: 0.9944 - val_loss: 0.0563 - val_accuracy: 0.9849\n",
      "Epoch 16/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0158 - accuracy: 0.9951 - val_loss: 0.0538 - val_accuracy: 0.9866\n",
      "Epoch 17/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0136 - accuracy: 0.9953 - val_loss: 0.0521 - val_accuracy: 0.9863\n",
      "Epoch 18/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0127 - accuracy: 0.9957 - val_loss: 0.0611 - val_accuracy: 0.9856\n",
      "Epoch 19/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0111 - accuracy: 0.9961 - val_loss: 0.0502 - val_accuracy: 0.9867\n",
      "Epoch 20/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0107 - accuracy: 0.9966 - val_loss: 0.0544 - val_accuracy: 0.9860\n",
      "Epoch 21/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0094 - accuracy: 0.9967 - val_loss: 0.0680 - val_accuracy: 0.9848\n",
      "Epoch 22/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0099 - accuracy: 0.9967 - val_loss: 0.0581 - val_accuracy: 0.9867\n",
      "Epoch 23/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.0635 - val_accuracy: 0.9853\n",
      "Epoch 24/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0075 - accuracy: 0.9972 - val_loss: 0.0748 - val_accuracy: 0.9839\n",
      "Epoch 25/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.0662 - val_accuracy: 0.9863\n",
      "Epoch 26/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.0621 - val_accuracy: 0.9866\n",
      "Epoch 27/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0056 - accuracy: 0.9980 - val_loss: 0.0749 - val_accuracy: 0.9837\n",
      "Epoch 28/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0051 - accuracy: 0.9981 - val_loss: 0.0698 - val_accuracy: 0.9862\n",
      "Epoch 29/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0077 - accuracy: 0.9974 - val_loss: 0.0671 - val_accuracy: 0.9858\n",
      "Epoch 30/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0765 - val_accuracy: 0.9851\n",
      "Epoch 31/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0064 - accuracy: 0.9978 - val_loss: 0.0639 - val_accuracy: 0.9866\n",
      "Epoch 32/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.0749 - val_accuracy: 0.9854\n",
      "Epoch 33/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0029 - accuracy: 0.9989 - val_loss: 0.0709 - val_accuracy: 0.9867\n",
      "Epoch 34/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0084 - accuracy: 0.9973 - val_loss: 0.0718 - val_accuracy: 0.9862\n",
      "Epoch 35/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0687 - val_accuracy: 0.9867\n",
      "Epoch 36/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0702 - val_accuracy: 0.9877\n",
      "Epoch 37/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0693 - val_accuracy: 0.9878\n",
      "Epoch 38/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0733 - val_accuracy: 0.9879\n",
      "Epoch 39/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.0708 - val_accuracy: 0.9870\n",
      "Epoch 40/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0090 - accuracy: 0.9968 - val_loss: 0.0713 - val_accuracy: 0.9858\n",
      "Epoch 41/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0724 - val_accuracy: 0.9859\n",
      "Epoch 42/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0705 - val_accuracy: 0.9871\n",
      "Epoch 43/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0705 - val_accuracy: 0.9871\n",
      "Epoch 44/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0690 - val_accuracy: 0.9864\n",
      "Epoch 45/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0752 - val_accuracy: 0.9869\n",
      "Epoch 46/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 4.1863e-04 - accuracy: 0.9999 - val_loss: 0.0757 - val_accuracy: 0.9872\n",
      "Epoch 47/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.2636e-04 - accuracy: 1.0000 - val_loss: 0.0755 - val_accuracy: 0.9877\n",
      "Epoch 48/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 6.8697e-05 - accuracy: 1.0000 - val_loss: 0.0765 - val_accuracy: 0.9880\n",
      "Epoch 49/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 6.1212e-05 - accuracy: 1.0000 - val_loss: 0.0809 - val_accuracy: 0.9876\n",
      "Epoch 50/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0130 - accuracy: 0.9956 - val_loss: 0.0621 - val_accuracy: 0.9861\n",
      "Epoch 51/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0040 - accuracy: 0.9986 - val_loss: 0.0801 - val_accuracy: 0.9847\n",
      "Epoch 52/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0729 - val_accuracy: 0.9873\n",
      "Epoch 53/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0723 - val_accuracy: 0.9881\n",
      "Epoch 54/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0876 - val_accuracy: 0.9865\n",
      "Epoch 55/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0049 - accuracy: 0.9985 - val_loss: 0.0769 - val_accuracy: 0.9867\n",
      "Epoch 56/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0681 - val_accuracy: 0.9883\n",
      "Epoch 57/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.0409e-04 - accuracy: 1.0000 - val_loss: 0.0714 - val_accuracy: 0.9880\n",
      "Epoch 58/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.6169e-04 - accuracy: 1.0000 - val_loss: 0.0703 - val_accuracy: 0.9885\n",
      "Epoch 59/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 5.5055e-05 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9884\n",
      "Epoch 60/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.9545e-05 - accuracy: 1.0000 - val_loss: 0.0742 - val_accuracy: 0.9886\n",
      "Epoch 61/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.3099e-05 - accuracy: 1.0000 - val_loss: 0.0770 - val_accuracy: 0.9884\n",
      "Epoch 62/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.9135e-05 - accuracy: 1.0000 - val_loss: 0.0782 - val_accuracy: 0.9886\n",
      "Epoch 63/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.6347e-05 - accuracy: 1.0000 - val_loss: 0.0792 - val_accuracy: 0.9885\n",
      "Epoch 64/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.3488e-05 - accuracy: 1.0000 - val_loss: 0.0806 - val_accuracy: 0.9883\n",
      "Epoch 65/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.0851e-05 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 0.9883\n",
      "Epoch 66/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 9.2148e-06 - accuracy: 1.0000 - val_loss: 0.0844 - val_accuracy: 0.9883\n",
      "Epoch 67/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 7.8521e-06 - accuracy: 1.0000 - val_loss: 0.0854 - val_accuracy: 0.9883\n",
      "Epoch 68/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 6.4744e-06 - accuracy: 1.0000 - val_loss: 0.0868 - val_accuracy: 0.9881\n",
      "Epoch 69/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 5.4356e-06 - accuracy: 1.0000 - val_loss: 0.0881 - val_accuracy: 0.9885\n",
      "Epoch 70/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 4.8239e-06 - accuracy: 1.0000 - val_loss: 0.0904 - val_accuracy: 0.9882\n",
      "Epoch 71/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.8524e-06 - accuracy: 1.0000 - val_loss: 0.0913 - val_accuracy: 0.9882\n",
      "Epoch 72/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.2491e-06 - accuracy: 1.0000 - val_loss: 0.0928 - val_accuracy: 0.9881\n",
      "Epoch 73/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.7953e-06 - accuracy: 1.0000 - val_loss: 0.0950 - val_accuracy: 0.9882\n",
      "Epoch 74/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.4357e-06 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9883\n",
      "Epoch 75/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.9301e-06 - accuracy: 1.0000 - val_loss: 0.0974 - val_accuracy: 0.9883\n",
      "Epoch 76/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.8432e-06 - accuracy: 1.0000 - val_loss: 0.1004 - val_accuracy: 0.9882\n",
      "Epoch 77/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.5433e-06 - accuracy: 1.0000 - val_loss: 0.1006 - val_accuracy: 0.9883\n",
      "Epoch 78/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.1497e-06 - accuracy: 1.0000 - val_loss: 0.1024 - val_accuracy: 0.9881\n",
      "Epoch 79/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.0252e-06 - accuracy: 1.0000 - val_loss: 0.1047 - val_accuracy: 0.9880\n",
      "Epoch 80/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 7.8921e-07 - accuracy: 1.0000 - val_loss: 0.1059 - val_accuracy: 0.9881\n",
      "Epoch 81/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 6.8422e-07 - accuracy: 1.0000 - val_loss: 0.1070 - val_accuracy: 0.9881\n",
      "Epoch 82/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 5.9731e-07 - accuracy: 1.0000 - val_loss: 0.1091 - val_accuracy: 0.9883\n",
      "Epoch 83/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 4.7429e-07 - accuracy: 1.0000 - val_loss: 0.1108 - val_accuracy: 0.9883\n",
      "Epoch 84/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.9577e-07 - accuracy: 1.0000 - val_loss: 0.1118 - val_accuracy: 0.9884\n",
      "Epoch 85/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.6875e-07 - accuracy: 1.0000 - val_loss: 0.1142 - val_accuracy: 0.9883\n",
      "Epoch 86/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.8301e-07 - accuracy: 1.0000 - val_loss: 0.1155 - val_accuracy: 0.9883\n",
      "Epoch 87/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.3799e-07 - accuracy: 1.0000 - val_loss: 0.1175 - val_accuracy: 0.9884\n",
      "Epoch 88/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.0884e-07 - accuracy: 1.0000 - val_loss: 0.1203 - val_accuracy: 0.9883\n",
      "Epoch 89/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.1990e-07 - accuracy: 1.0000 - val_loss: 0.1214 - val_accuracy: 0.9883\n",
      "Epoch 90/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.0931e-07 - accuracy: 1.0000 - val_loss: 0.1232 - val_accuracy: 0.9884\n",
      "Epoch 91/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.1874e-07 - accuracy: 1.0000 - val_loss: 0.1224 - val_accuracy: 0.9886\n",
      "Epoch 92/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0261 - accuracy: 0.9952 - val_loss: 0.0685 - val_accuracy: 0.9849\n",
      "Epoch 93/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0689 - val_accuracy: 0.9876\n",
      "Epoch 94/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 8.3208e-04 - accuracy: 0.9998 - val_loss: 0.0729 - val_accuracy: 0.9876\n",
      "Epoch 95/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 5.3594e-04 - accuracy: 0.9999 - val_loss: 0.0731 - val_accuracy: 0.9880\n",
      "Epoch 96/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 8.5168e-05 - accuracy: 1.0000 - val_loss: 0.0749 - val_accuracy: 0.9882\n",
      "Epoch 97/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 5.6261e-05 - accuracy: 1.0000 - val_loss: 0.0759 - val_accuracy: 0.9882\n",
      "Epoch 98/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 4.1921e-05 - accuracy: 1.0000 - val_loss: 0.0773 - val_accuracy: 0.9883\n",
      "Epoch 99/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.1442e-05 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 0.9884\n",
      "Epoch 100/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.5116e-05 - accuracy: 1.0000 - val_loss: 0.0804 - val_accuracy: 0.9884\n",
      "Epoch 101/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.0086e-05 - accuracy: 1.0000 - val_loss: 0.0815 - val_accuracy: 0.9887\n",
      "Epoch 102/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.6714e-05 - accuracy: 1.0000 - val_loss: 0.0830 - val_accuracy: 0.9886\n",
      "Epoch 103/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.3705e-05 - accuracy: 1.0000 - val_loss: 0.0843 - val_accuracy: 0.9886\n",
      "Epoch 104/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.0932e-05 - accuracy: 1.0000 - val_loss: 0.0856 - val_accuracy: 0.9885\n",
      "Epoch 105/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 9.2216e-06 - accuracy: 1.0000 - val_loss: 0.0867 - val_accuracy: 0.9886\n",
      "Epoch 106/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 7.5700e-06 - accuracy: 1.0000 - val_loss: 0.0884 - val_accuracy: 0.9883\n",
      "Epoch 107/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 6.4283e-06 - accuracy: 1.0000 - val_loss: 0.0892 - val_accuracy: 0.9884\n",
      "Epoch 108/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 5.2808e-06 - accuracy: 1.0000 - val_loss: 0.0903 - val_accuracy: 0.9884\n",
      "Epoch 109/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 4.5264e-06 - accuracy: 1.0000 - val_loss: 0.0918 - val_accuracy: 0.9885\n",
      "Epoch 110/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.7152e-06 - accuracy: 1.0000 - val_loss: 0.0934 - val_accuracy: 0.9886\n",
      "Epoch 111/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.1105e-06 - accuracy: 1.0000 - val_loss: 0.0946 - val_accuracy: 0.9886\n",
      "Epoch 112/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.5772e-06 - accuracy: 1.0000 - val_loss: 0.0957 - val_accuracy: 0.9884\n",
      "Epoch 113/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.2525e-06 - accuracy: 1.0000 - val_loss: 0.0970 - val_accuracy: 0.9886\n",
      "Epoch 114/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.8655e-06 - accuracy: 1.0000 - val_loss: 0.0985 - val_accuracy: 0.9885\n",
      "Epoch 115/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.6253e-06 - accuracy: 1.0000 - val_loss: 0.1005 - val_accuracy: 0.9884\n",
      "Epoch 116/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.4210e-06 - accuracy: 1.0000 - val_loss: 0.1013 - val_accuracy: 0.9885\n",
      "Epoch 117/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.1418e-06 - accuracy: 1.0000 - val_loss: 0.1027 - val_accuracy: 0.9886\n",
      "Epoch 118/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 9.6953e-07 - accuracy: 1.0000 - val_loss: 0.1041 - val_accuracy: 0.9885\n",
      "Epoch 119/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 7.9842e-07 - accuracy: 1.0000 - val_loss: 0.1055 - val_accuracy: 0.9885\n",
      "Epoch 120/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 6.7049e-07 - accuracy: 1.0000 - val_loss: 0.1070 - val_accuracy: 0.9886\n",
      "Epoch 121/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 5.6263e-07 - accuracy: 1.0000 - val_loss: 0.1080 - val_accuracy: 0.9886\n",
      "Epoch 122/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 4.7423e-07 - accuracy: 1.0000 - val_loss: 0.1095 - val_accuracy: 0.9884\n",
      "Epoch 123/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 4.0617e-07 - accuracy: 1.0000 - val_loss: 0.1113 - val_accuracy: 0.9884\n",
      "Epoch 124/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.4357e-07 - accuracy: 1.0000 - val_loss: 0.1125 - val_accuracy: 0.9885\n",
      "Epoch 125/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.8707e-07 - accuracy: 1.0000 - val_loss: 0.1137 - val_accuracy: 0.9885\n",
      "Epoch 126/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.4299e-07 - accuracy: 1.0000 - val_loss: 0.1150 - val_accuracy: 0.9886\n",
      "Epoch 127/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.0488e-07 - accuracy: 1.0000 - val_loss: 0.1167 - val_accuracy: 0.9884\n",
      "Epoch 128/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.7316e-07 - accuracy: 1.0000 - val_loss: 0.1187 - val_accuracy: 0.9884\n",
      "Epoch 129/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.5145e-07 - accuracy: 1.0000 - val_loss: 0.1192 - val_accuracy: 0.9884\n",
      "Epoch 130/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.2437e-07 - accuracy: 1.0000 - val_loss: 0.1209 - val_accuracy: 0.9885\n",
      "Epoch 131/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.0508e-07 - accuracy: 1.0000 - val_loss: 0.1223 - val_accuracy: 0.9886\n",
      "Epoch 132/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 8.8951e-08 - accuracy: 1.0000 - val_loss: 0.1239 - val_accuracy: 0.9884\n",
      "Epoch 133/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 8.2188e-08 - accuracy: 1.0000 - val_loss: 0.1260 - val_accuracy: 0.9883\n",
      "Epoch 134/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0205 - accuracy: 0.9959 - val_loss: 0.0801 - val_accuracy: 0.9858\n",
      "Epoch 135/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 0.0055 - accuracy: 0.9986 - val_loss: 0.0692 - val_accuracy: 0.9884\n",
      "Epoch 136/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 5.1579e-04 - accuracy: 0.9998 - val_loss: 0.0686 - val_accuracy: 0.9884\n",
      "Epoch 137/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.0193e-04 - accuracy: 1.0000 - val_loss: 0.0714 - val_accuracy: 0.9891\n",
      "Epoch 138/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 5.2828e-05 - accuracy: 1.0000 - val_loss: 0.0728 - val_accuracy: 0.9891\n",
      "Epoch 139/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.8994e-05 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 0.9891\n",
      "Epoch 140/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.0848e-05 - accuracy: 1.0000 - val_loss: 0.0755 - val_accuracy: 0.9890\n",
      "Epoch 141/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.4607e-05 - accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 0.9889\n",
      "Epoch 142/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.0232e-05 - accuracy: 1.0000 - val_loss: 0.0783 - val_accuracy: 0.9888\n",
      "Epoch 143/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.6656e-05 - accuracy: 1.0000 - val_loss: 0.0797 - val_accuracy: 0.9888\n",
      "Epoch 144/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.4183e-05 - accuracy: 1.0000 - val_loss: 0.0807 - val_accuracy: 0.9888\n",
      "Epoch 145/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.1622e-05 - accuracy: 1.0000 - val_loss: 0.0818 - val_accuracy: 0.9890\n",
      "Epoch 146/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 9.6377e-06 - accuracy: 1.0000 - val_loss: 0.0830 - val_accuracy: 0.9891\n",
      "Epoch 147/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 8.2940e-06 - accuracy: 1.0000 - val_loss: 0.0847 - val_accuracy: 0.9889\n",
      "Epoch 148/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 6.6977e-06 - accuracy: 1.0000 - val_loss: 0.0863 - val_accuracy: 0.9888\n",
      "Epoch 149/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 5.7238e-06 - accuracy: 1.0000 - val_loss: 0.0869 - val_accuracy: 0.9889\n",
      "Epoch 150/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 4.6740e-06 - accuracy: 1.0000 - val_loss: 0.0881 - val_accuracy: 0.9888\n",
      "Epoch 151/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.9423e-06 - accuracy: 1.0000 - val_loss: 0.0897 - val_accuracy: 0.9889\n",
      "Epoch 152/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.2810e-06 - accuracy: 1.0000 - val_loss: 0.0906 - val_accuracy: 0.9889\n",
      "Epoch 153/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.7203e-06 - accuracy: 1.0000 - val_loss: 0.0925 - val_accuracy: 0.9889\n",
      "Epoch 154/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.3533e-06 - accuracy: 1.0000 - val_loss: 0.0941 - val_accuracy: 0.9890\n",
      "Epoch 155/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.9553e-06 - accuracy: 1.0000 - val_loss: 0.0952 - val_accuracy: 0.9891\n",
      "Epoch 156/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.6037e-06 - accuracy: 1.0000 - val_loss: 0.0969 - val_accuracy: 0.9891\n",
      "Epoch 157/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.3708e-06 - accuracy: 1.0000 - val_loss: 0.0977 - val_accuracy: 0.9891\n",
      "Epoch 158/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.1265e-06 - accuracy: 1.0000 - val_loss: 0.0994 - val_accuracy: 0.9891\n",
      "Epoch 159/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 9.2099e-07 - accuracy: 1.0000 - val_loss: 0.1009 - val_accuracy: 0.9890\n",
      "Epoch 160/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 7.8208e-07 - accuracy: 1.0000 - val_loss: 0.1026 - val_accuracy: 0.9889\n",
      "Epoch 161/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 6.9885e-07 - accuracy: 1.0000 - val_loss: 0.1034 - val_accuracy: 0.9892\n",
      "Epoch 162/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 6.4069e-07 - accuracy: 1.0000 - val_loss: 0.1048 - val_accuracy: 0.9892\n",
      "Epoch 163/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 4.8160e-07 - accuracy: 1.0000 - val_loss: 0.1061 - val_accuracy: 0.9891\n",
      "Epoch 164/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.8577e-07 - accuracy: 1.0000 - val_loss: 0.1075 - val_accuracy: 0.9892\n",
      "Epoch 165/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.2289e-07 - accuracy: 1.0000 - val_loss: 0.1091 - val_accuracy: 0.9890\n",
      "Epoch 166/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.7731e-07 - accuracy: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.9891\n",
      "Epoch 167/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.3006e-07 - accuracy: 1.0000 - val_loss: 0.1119 - val_accuracy: 0.9892\n",
      "Epoch 168/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.9003e-07 - accuracy: 1.0000 - val_loss: 0.1139 - val_accuracy: 0.9889\n",
      "Epoch 169/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.6559e-07 - accuracy: 1.0000 - val_loss: 0.1150 - val_accuracy: 0.9890\n",
      "Epoch 170/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.3297e-07 - accuracy: 1.0000 - val_loss: 0.1160 - val_accuracy: 0.9891\n",
      "Epoch 171/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.1651e-07 - accuracy: 1.0000 - val_loss: 0.1179 - val_accuracy: 0.9889\n",
      "Epoch 172/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 9.5051e-08 - accuracy: 1.0000 - val_loss: 0.1189 - val_accuracy: 0.9890\n",
      "Epoch 173/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 8.1039e-08 - accuracy: 1.0000 - val_loss: 0.1208 - val_accuracy: 0.9890\n",
      "Epoch 174/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 6.8612e-08 - accuracy: 1.0000 - val_loss: 0.1219 - val_accuracy: 0.9893\n",
      "Epoch 175/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 5.7920e-08 - accuracy: 1.0000 - val_loss: 0.1235 - val_accuracy: 0.9891\n",
      "Epoch 176/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 5.0832e-08 - accuracy: 1.0000 - val_loss: 0.1251 - val_accuracy: 0.9891\n",
      "Epoch 177/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 4.0896e-08 - accuracy: 1.0000 - val_loss: 0.1261 - val_accuracy: 0.9891\n",
      "Epoch 178/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.5396e-08 - accuracy: 1.0000 - val_loss: 0.1276 - val_accuracy: 0.9891\n",
      "Epoch 179/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.9870e-08 - accuracy: 1.0000 - val_loss: 0.1294 - val_accuracy: 0.9892\n",
      "Epoch 180/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.4704e-08 - accuracy: 1.0000 - val_loss: 0.1310 - val_accuracy: 0.9892\n",
      "Epoch 181/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.1129e-08 - accuracy: 1.0000 - val_loss: 0.1323 - val_accuracy: 0.9891\n",
      "Epoch 182/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.8193e-08 - accuracy: 1.0000 - val_loss: 0.1327 - val_accuracy: 0.9892\n",
      "Epoch 183/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.5685e-08 - accuracy: 1.0000 - val_loss: 0.1342 - val_accuracy: 0.9891\n",
      "Epoch 184/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.3248e-08 - accuracy: 1.0000 - val_loss: 0.1364 - val_accuracy: 0.9891\n",
      "Epoch 185/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.1465e-08 - accuracy: 1.0000 - val_loss: 0.1371 - val_accuracy: 0.9892\n",
      "Epoch 186/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 9.7124e-09 - accuracy: 1.0000 - val_loss: 0.1385 - val_accuracy: 0.9891\n",
      "Epoch 187/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 8.8348e-09 - accuracy: 1.0000 - val_loss: 0.1403 - val_accuracy: 0.9890\n",
      "Epoch 188/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 7.7789e-09 - accuracy: 1.0000 - val_loss: 0.1410 - val_accuracy: 0.9890\n",
      "Epoch 189/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 6.2617e-09 - accuracy: 1.0000 - val_loss: 0.1422 - val_accuracy: 0.9891\n",
      "Epoch 190/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 5.3105e-09 - accuracy: 1.0000 - val_loss: 0.1433 - val_accuracy: 0.9891\n",
      "Epoch 191/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 4.5660e-09 - accuracy: 1.0000 - val_loss: 0.1451 - val_accuracy: 0.9889\n",
      "Epoch 192/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.8640e-09 - accuracy: 1.0000 - val_loss: 0.1460 - val_accuracy: 0.9891\n",
      "Epoch 193/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 3.4196e-09 - accuracy: 1.0000 - val_loss: 0.1471 - val_accuracy: 0.9890\n",
      "Epoch 194/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.9497e-09 - accuracy: 1.0000 - val_loss: 0.1482 - val_accuracy: 0.9889\n",
      "Epoch 195/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.6581e-09 - accuracy: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.9889\n",
      "Epoch 196/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.2674e-09 - accuracy: 1.0000 - val_loss: 0.1506 - val_accuracy: 0.9890\n",
      "Epoch 197/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 2.0127e-09 - accuracy: 1.0000 - val_loss: 0.1515 - val_accuracy: 0.9891\n",
      "Epoch 198/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.7409e-09 - accuracy: 1.0000 - val_loss: 0.1519 - val_accuracy: 0.9891\n",
      "Epoch 199/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.5937e-09 - accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9889\n",
      "Epoch 200/200\n",
      "329/329 [==============================] - 1s 4ms/step - loss: 1.4324e-09 - accuracy: 1.0000 - val_loss: 0.1540 - val_accuracy: 0.9891\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 200\n",
    "\n",
    "history_LeNet5 = model.fit(X_train_LeNet5, Y_train_LeNet5,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_valid_LeNet5, Y_valid_LeNet5))\n",
    "          #callbacks=[learning_rate_reduction,es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wC8Hoxf0ajjn"
   },
   "source": [
    "##### ***Evaluating the Model***\n",
    "\n",
    "From the learning curve graph below, It can be noticed that there is a fluctuation of validation accuracy and validation loss while training the model. It seems the SGD optimiser does not work well with this model. However, it provided the validation accuracy at 98.91%. In the following part, we will apply the data augmentation and observe the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "JFzA6Q_AV0fb",
    "outputId": "25f512a0-0881-4bc4-ea95-f907d157e078"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAGFCAYAAADU/MRFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ne0nU/PFgeAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Learning curve\n",
    "pd.DataFrame(history_LeNet5.history).plot(figsize=(12, 6))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title('Learning Curve', fontsize=15)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BIeQYlIdkUIk"
   },
   "source": [
    "##### ***Making prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "nEYQvzhmIyC8",
    "outputId": "e0e1b9d2-662a-429a-bf6a-4062fc39f96a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 9])"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test = model.predict_classes(X_test_LeNet5)\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ugpucTHHZpq"
   },
   "source": [
    "#### **3.3.4 Improving the accuracy by using Data Augmentation technique**\n",
    "\n",
    "Data augmentation is used to increase the size of the training set by creating a several practical variants of each training sample. Furthermore, this is a regularisation technique that can reduce over-fitting. There are many ways to generate a new image, such as random cropping, rotation, translation, rescale, shearing, colour shifting, etc. Hence, the \"ImageDataGenerator\" fuction from Keras is used for data augmentation technique. \n",
    "\n",
    "[More details on ImageDataGenerator and their parameters](https://keras.io/preprocessing/image/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "ndVJcqeXHYU3",
    "outputId": "b9fb23b0-56a9-4d44-c8be-ab74b256e1db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_augmentation = ImageDataGenerator(rotation_range = 10,\n",
    "                                   width_shift_range = 0.25,\n",
    "                                   height_shift_range = 0.25,\n",
    "                                   shear_range = 0.1,\n",
    "                                   zoom_range = 0.25,\n",
    "                                   horizontal_flip = False)\n",
    "\n",
    "valid_augmentation = ImageDataGenerator() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J1flRENhCKaP"
   },
   "source": [
    "##### ***Early Stopping***\n",
    "Early stopping method will be applied when the model is being fit. This technique is to stop training as soon as the validation error reaches a minimum. It is one of the most efficient regularisation method Geoffrey Hinton called it as a \"beautiful free lunch\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gKGxQPPNIyC7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=300, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w385HI0tU4jp"
   },
   "source": [
    "##### ***Learning Rate Reduction***\n",
    "The learning rate reduction is used to make the optimiser converge faster and closer to the global minimum. This function can help to decrease the learning rate during the training to meet the global loss function minimum efficiently. \n",
    "\n",
    "We reduce the learning rate by half if the accuracy does not improve after 3 epochs using the ReduceLROnPlateau function from Keras.callbacks to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "e0hHJIkgD__r"
   },
   "outputs": [],
   "source": [
    "# Set a learning rate reduction\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0c8FwuHBf0-S"
   },
   "source": [
    "##### ***Fitting the Model***\n",
    "This experiment used model, parameters and optimiser from the previous section. You will see the improvement of the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "fH4eV9A6euhh",
    "outputId": "eefb5293-1cb4-4bf8-e6bf-ab77d59a20ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 6)         60        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 15, 15, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 16)        880       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 6, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               69240     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 81,194\n",
      "Trainable params: 81,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Perform layers\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(32,32,1)),\n",
    "    tf.keras.layers.AveragePooling2D(),\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.AveragePooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=120, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=84, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Displays all model's layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "W3lCz75jHYPo",
    "outputId": "385edd7c-878f-44fc-d229-cef1cad64ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-e17cba0d885e>:10: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 1.2159 - accuracy: 0.5915 - val_loss: 0.4076 - val_accuracy: 0.8823 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.5780 - accuracy: 0.8203 - val_loss: 0.2495 - val_accuracy: 0.9273 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.4626 - accuracy: 0.8562 - val_loss: 0.1830 - val_accuracy: 0.9493 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.3972 - accuracy: 0.8771 - val_loss: 0.1720 - val_accuracy: 0.9537 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.3623 - accuracy: 0.8862 - val_loss: 0.1641 - val_accuracy: 0.9511 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.3324 - accuracy: 0.8971 - val_loss: 0.1413 - val_accuracy: 0.9593 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.3099 - accuracy: 0.9032 - val_loss: 0.1408 - val_accuracy: 0.9577 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.2916 - accuracy: 0.9087 - val_loss: 0.1159 - val_accuracy: 0.9651 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.2764 - accuracy: 0.9125 - val_loss: 0.1127 - val_accuracy: 0.9682 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.2575 - accuracy: 0.9178 - val_loss: 0.1163 - val_accuracy: 0.9657 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.2523 - accuracy: 0.9194 - val_loss: 0.1129 - val_accuracy: 0.9651 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.2410 - accuracy: 0.9232 - val_loss: 0.0912 - val_accuracy: 0.9741 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.2329 - accuracy: 0.9254 - val_loss: 0.1282 - val_accuracy: 0.9598 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.2253 - accuracy: 0.9273 - val_loss: 0.0938 - val_accuracy: 0.9724 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.2175 - accuracy: 0.9306 - val_loss: 0.1216 - val_accuracy: 0.9654 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.2111 - accuracy: 0.9329 - val_loss: 0.1001 - val_accuracy: 0.9712 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.2092 - accuracy: 0.9326 - val_loss: 0.1144 - val_accuracy: 0.9654 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.2028 - accuracy: 0.9357 - val_loss: 0.0786 - val_accuracy: 0.9769 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1977 - accuracy: 0.9371 - val_loss: 0.0958 - val_accuracy: 0.9723 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1944 - accuracy: 0.9371 - val_loss: 0.0870 - val_accuracy: 0.9743 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1933 - accuracy: 0.9381 - val_loss: 0.0912 - val_accuracy: 0.9743 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1829 - accuracy: 0.9412 - val_loss: 0.0980 - val_accuracy: 0.9714 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1841 - accuracy: 0.9400 - val_loss: 0.0825 - val_accuracy: 0.9772 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1822 - accuracy: 0.9418 - val_loss: 0.0795 - val_accuracy: 0.9768 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1759 - accuracy: 0.9433 - val_loss: 0.0789 - val_accuracy: 0.9752 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1738 - accuracy: 0.9439 - val_loss: 0.0821 - val_accuracy: 0.9753 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1679 - accuracy: 0.9458 - val_loss: 0.0944 - val_accuracy: 0.9718 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1682 - accuracy: 0.9446 - val_loss: 0.0724 - val_accuracy: 0.9786 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1641 - accuracy: 0.9459 - val_loss: 0.0764 - val_accuracy: 0.9766 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1620 - accuracy: 0.9463 - val_loss: 0.0679 - val_accuracy: 0.9796 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1619 - accuracy: 0.9463 - val_loss: 0.0743 - val_accuracy: 0.9776 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1619 - accuracy: 0.9468 - val_loss: 0.0650 - val_accuracy: 0.9798 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1586 - accuracy: 0.9489 - val_loss: 0.0747 - val_accuracy: 0.9779 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1544 - accuracy: 0.9488 - val_loss: 0.0707 - val_accuracy: 0.9779 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1532 - accuracy: 0.9489 - val_loss: 0.0654 - val_accuracy: 0.9812 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1446 - accuracy: 0.9528 - val_loss: 0.0632 - val_accuracy: 0.9805 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1480 - accuracy: 0.9510 - val_loss: 0.0867 - val_accuracy: 0.9750 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1404 - accuracy: 0.9538 - val_loss: 0.0843 - val_accuracy: 0.9759 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1491 - accuracy: 0.9521 - val_loss: 0.0731 - val_accuracy: 0.9783 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1400 - accuracy: 0.9540 - val_loss: 0.0672 - val_accuracy: 0.9802 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1455 - accuracy: 0.9525 - val_loss: 0.0669 - val_accuracy: 0.9811 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1381 - accuracy: 0.9545 - val_loss: 0.0574 - val_accuracy: 0.9827 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1386 - accuracy: 0.9543 - val_loss: 0.0568 - val_accuracy: 0.9824 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1367 - accuracy: 0.9556 - val_loss: 0.0561 - val_accuracy: 0.9840 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1385 - accuracy: 0.9546 - val_loss: 0.0530 - val_accuracy: 0.9856 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1327 - accuracy: 0.9557 - val_loss: 0.0570 - val_accuracy: 0.9825 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1363 - accuracy: 0.9550 - val_loss: 0.0572 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1285 - accuracy: 0.9587 - val_loss: 0.0581 - val_accuracy: 0.9822 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1316 - accuracy: 0.9569 - val_loss: 0.0589 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1257 - accuracy: 0.9587 - val_loss: 0.0638 - val_accuracy: 0.9809 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1263 - accuracy: 0.9585 - val_loss: 0.0497 - val_accuracy: 0.9852 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1270 - accuracy: 0.9570 - val_loss: 0.0607 - val_accuracy: 0.9807 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1241 - accuracy: 0.9589 - val_loss: 0.0669 - val_accuracy: 0.9801 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1243 - accuracy: 0.9601 - val_loss: 0.0635 - val_accuracy: 0.9807 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1230 - accuracy: 0.9605 - val_loss: 0.0697 - val_accuracy: 0.9796 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1248 - accuracy: 0.9594 - val_loss: 0.0517 - val_accuracy: 0.9854 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1213 - accuracy: 0.9608 - val_loss: 0.0554 - val_accuracy: 0.9835 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1246 - accuracy: 0.9594 - val_loss: 0.0519 - val_accuracy: 0.9843 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1211 - accuracy: 0.9600 - val_loss: 0.0538 - val_accuracy: 0.9833 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1177 - accuracy: 0.9602 - val_loss: 0.0533 - val_accuracy: 0.9833 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1189 - accuracy: 0.9614 - val_loss: 0.0511 - val_accuracy: 0.9848 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1185 - accuracy: 0.9606 - val_loss: 0.0542 - val_accuracy: 0.9843 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1156 - accuracy: 0.9615 - val_loss: 0.0519 - val_accuracy: 0.9849 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1096 - accuracy: 0.9637 - val_loss: 0.0449 - val_accuracy: 0.9866 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1149 - accuracy: 0.9622 - val_loss: 0.0471 - val_accuracy: 0.9859 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1123 - accuracy: 0.9616 - val_loss: 0.0503 - val_accuracy: 0.9857 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1117 - accuracy: 0.9623 - val_loss: 0.0504 - val_accuracy: 0.9841 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1116 - accuracy: 0.9629 - val_loss: 0.0513 - val_accuracy: 0.9839 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1154 - accuracy: 0.9623 - val_loss: 0.0498 - val_accuracy: 0.9838 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1086 - accuracy: 0.9645 - val_loss: 0.0472 - val_accuracy: 0.9854 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1102 - accuracy: 0.9626 - val_loss: 0.0539 - val_accuracy: 0.9838 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1087 - accuracy: 0.9637 - val_loss: 0.0531 - val_accuracy: 0.9835 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1099 - accuracy: 0.9629 - val_loss: 0.0473 - val_accuracy: 0.9852 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1052 - accuracy: 0.9652 - val_loss: 0.0450 - val_accuracy: 0.9868 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1051 - accuracy: 0.9642 - val_loss: 0.0396 - val_accuracy: 0.9883 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1081 - accuracy: 0.9632 - val_loss: 0.0415 - val_accuracy: 0.9879 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1019 - accuracy: 0.9653 - val_loss: 0.0417 - val_accuracy: 0.9874 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1050 - accuracy: 0.9648 - val_loss: 0.0452 - val_accuracy: 0.9853 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1043 - accuracy: 0.9656 - val_loss: 0.0486 - val_accuracy: 0.9855 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1009 - accuracy: 0.9664 - val_loss: 0.0383 - val_accuracy: 0.9893 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1067 - accuracy: 0.9656 - val_loss: 0.0401 - val_accuracy: 0.9880 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1000 - accuracy: 0.9671 - val_loss: 0.0475 - val_accuracy: 0.9858 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 9s - loss: 0.0976 - accuracy: 0.9677 - val_loss: 0.0435 - val_accuracy: 0.9875 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0999 - accuracy: 0.9672 - val_loss: 0.0424 - val_accuracy: 0.9866 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0992 - accuracy: 0.9667 - val_loss: 0.0435 - val_accuracy: 0.9876 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1026 - accuracy: 0.9662 - val_loss: 0.0389 - val_accuracy: 0.9887 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0983 - accuracy: 0.9679 - val_loss: 0.0400 - val_accuracy: 0.9877 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.1004 - accuracy: 0.9660 - val_loss: 0.0431 - val_accuracy: 0.9867 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0969 - accuracy: 0.9675 - val_loss: 0.0381 - val_accuracy: 0.9882 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0968 - accuracy: 0.9676 - val_loss: 0.0411 - val_accuracy: 0.9878 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0942 - accuracy: 0.9684 - val_loss: 0.0419 - val_accuracy: 0.9878 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0954 - accuracy: 0.9683 - val_loss: 0.0445 - val_accuracy: 0.9873 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0991 - accuracy: 0.9673 - val_loss: 0.0437 - val_accuracy: 0.9866 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0977 - accuracy: 0.9669 - val_loss: 0.0365 - val_accuracy: 0.9886 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0990 - accuracy: 0.9661 - val_loss: 0.0367 - val_accuracy: 0.9891 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0934 - accuracy: 0.9683 - val_loss: 0.0367 - val_accuracy: 0.9888 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0943 - accuracy: 0.9696 - val_loss: 0.0396 - val_accuracy: 0.9876 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0968 - accuracy: 0.9680 - val_loss: 0.0436 - val_accuracy: 0.9866 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0992 - accuracy: 0.9681 - val_loss: 0.0372 - val_accuracy: 0.9889 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0929 - accuracy: 0.9689 - val_loss: 0.0396 - val_accuracy: 0.9884 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0950 - accuracy: 0.9680 - val_loss: 0.0402 - val_accuracy: 0.9877 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0951 - accuracy: 0.9686 - val_loss: 0.0334 - val_accuracy: 0.9901 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0922 - accuracy: 0.9698 - val_loss: 0.0325 - val_accuracy: 0.9897 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0926 - accuracy: 0.9698 - val_loss: 0.0389 - val_accuracy: 0.9879 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0893 - accuracy: 0.9701 - val_loss: 0.0375 - val_accuracy: 0.9896 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0917 - accuracy: 0.9699 - val_loss: 0.0374 - val_accuracy: 0.9889 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0895 - accuracy: 0.9709 - val_loss: 0.0377 - val_accuracy: 0.9886 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0944 - accuracy: 0.9687 - val_loss: 0.0377 - val_accuracy: 0.9884 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0876 - accuracy: 0.9709 - val_loss: 0.0331 - val_accuracy: 0.9904 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0880 - accuracy: 0.9702 - val_loss: 0.0385 - val_accuracy: 0.9879 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0923 - accuracy: 0.9692 - val_loss: 0.0408 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0871 - accuracy: 0.9707 - val_loss: 0.0370 - val_accuracy: 0.9891 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0902 - accuracy: 0.9696 - val_loss: 0.0356 - val_accuracy: 0.9893 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0920 - accuracy: 0.9688 - val_loss: 0.0428 - val_accuracy: 0.9867 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0895 - accuracy: 0.9701 - val_loss: 0.0393 - val_accuracy: 0.9879 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0925 - accuracy: 0.9696 - val_loss: 0.0390 - val_accuracy: 0.9891 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0838 - accuracy: 0.9717 - val_loss: 0.0340 - val_accuracy: 0.9896 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0884 - accuracy: 0.9708 - val_loss: 0.0384 - val_accuracy: 0.9889 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0871 - accuracy: 0.9706 - val_loss: 0.0380 - val_accuracy: 0.9890 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0879 - accuracy: 0.9706 - val_loss: 0.0375 - val_accuracy: 0.9892 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0874 - accuracy: 0.9712 - val_loss: 0.0344 - val_accuracy: 0.9894 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0874 - accuracy: 0.9709 - val_loss: 0.0337 - val_accuracy: 0.9893 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0852 - accuracy: 0.9713 - val_loss: 0.0344 - val_accuracy: 0.9898 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0846 - accuracy: 0.9722 - val_loss: 0.0391 - val_accuracy: 0.9888 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0831 - accuracy: 0.9717 - val_loss: 0.0353 - val_accuracy: 0.9894 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0855 - accuracy: 0.9713 - val_loss: 0.0351 - val_accuracy: 0.9891 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0856 - accuracy: 0.9710 - val_loss: 0.0348 - val_accuracy: 0.9901 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0850 - accuracy: 0.9721 - val_loss: 0.0344 - val_accuracy: 0.9896 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0818 - accuracy: 0.9730 - val_loss: 0.0344 - val_accuracy: 0.9892 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0815 - accuracy: 0.9722 - val_loss: 0.0375 - val_accuracy: 0.9889 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0836 - accuracy: 0.9717 - val_loss: 0.0354 - val_accuracy: 0.9896 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0770 - accuracy: 0.9740 - val_loss: 0.0393 - val_accuracy: 0.9889 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0910 - accuracy: 0.9700 - val_loss: 0.0374 - val_accuracy: 0.9889 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0791 - accuracy: 0.9742 - val_loss: 0.0345 - val_accuracy: 0.9893 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0857 - accuracy: 0.9711 - val_loss: 0.0336 - val_accuracy: 0.9898 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0790 - accuracy: 0.9736 - val_loss: 0.0350 - val_accuracy: 0.9898 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0798 - accuracy: 0.9742 - val_loss: 0.0330 - val_accuracy: 0.9898 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0803 - accuracy: 0.9726 - val_loss: 0.0330 - val_accuracy: 0.9898 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0823 - accuracy: 0.9722 - val_loss: 0.0332 - val_accuracy: 0.9898 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0825 - accuracy: 0.9730 - val_loss: 0.0309 - val_accuracy: 0.9906 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0774 - accuracy: 0.9739 - val_loss: 0.0327 - val_accuracy: 0.9900 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0837 - accuracy: 0.9726 - val_loss: 0.0370 - val_accuracy: 0.9884 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0840 - accuracy: 0.9715 - val_loss: 0.0319 - val_accuracy: 0.9907 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0805 - accuracy: 0.9724 - val_loss: 0.0330 - val_accuracy: 0.9906 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0760 - accuracy: 0.9747 - val_loss: 0.0329 - val_accuracy: 0.9903 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0818 - accuracy: 0.9730 - val_loss: 0.0356 - val_accuracy: 0.9895 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0799 - accuracy: 0.9738 - val_loss: 0.0354 - val_accuracy: 0.9887 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0806 - accuracy: 0.9730 - val_loss: 0.0325 - val_accuracy: 0.9907 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0773 - accuracy: 0.9739 - val_loss: 0.0348 - val_accuracy: 0.9899 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0815 - accuracy: 0.9729 - val_loss: 0.0378 - val_accuracy: 0.9891 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0770 - accuracy: 0.9735 - val_loss: 0.0323 - val_accuracy: 0.9909 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0792 - accuracy: 0.9730 - val_loss: 0.0381 - val_accuracy: 0.9890 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0792 - accuracy: 0.9736 - val_loss: 0.0326 - val_accuracy: 0.9906 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0761 - accuracy: 0.9747 - val_loss: 0.0374 - val_accuracy: 0.9891 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0786 - accuracy: 0.9729 - val_loss: 0.0355 - val_accuracy: 0.9903 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0800 - accuracy: 0.9726 - val_loss: 0.0380 - val_accuracy: 0.9888 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0778 - accuracy: 0.9738 - val_loss: 0.0308 - val_accuracy: 0.9910 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0770 - accuracy: 0.9741 - val_loss: 0.0305 - val_accuracy: 0.9906 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0775 - accuracy: 0.9742 - val_loss: 0.0314 - val_accuracy: 0.9911 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0809 - accuracy: 0.9720 - val_loss: 0.0319 - val_accuracy: 0.9910 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0763 - accuracy: 0.9748 - val_loss: 0.0372 - val_accuracy: 0.9883 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0743 - accuracy: 0.9753 - val_loss: 0.0330 - val_accuracy: 0.9898 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0804 - accuracy: 0.9736 - val_loss: 0.0297 - val_accuracy: 0.9911 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0765 - accuracy: 0.9741 - val_loss: 0.0306 - val_accuracy: 0.9906 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0792 - accuracy: 0.9735 - val_loss: 0.0326 - val_accuracy: 0.9905 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0771 - accuracy: 0.9739 - val_loss: 0.0352 - val_accuracy: 0.9896 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0745 - accuracy: 0.9741 - val_loss: 0.0298 - val_accuracy: 0.9911 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0761 - accuracy: 0.9747 - val_loss: 0.0350 - val_accuracy: 0.9894 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0744 - accuracy: 0.9753 - val_loss: 0.0323 - val_accuracy: 0.9912 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0760 - accuracy: 0.9741 - val_loss: 0.0282 - val_accuracy: 0.9919 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0755 - accuracy: 0.9741 - val_loss: 0.0326 - val_accuracy: 0.9909 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0753 - accuracy: 0.9747 - val_loss: 0.0294 - val_accuracy: 0.9911 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0744 - accuracy: 0.9750 - val_loss: 0.0310 - val_accuracy: 0.9911 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0791 - accuracy: 0.9741 - val_loss: 0.0319 - val_accuracy: 0.9904 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0757 - accuracy: 0.9745 - val_loss: 0.0303 - val_accuracy: 0.9913 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0745 - accuracy: 0.9751 - val_loss: 0.0281 - val_accuracy: 0.9923 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0759 - accuracy: 0.9743 - val_loss: 0.0358 - val_accuracy: 0.9892 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0730 - accuracy: 0.9761 - val_loss: 0.0353 - val_accuracy: 0.9892 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0773 - accuracy: 0.9743 - val_loss: 0.0302 - val_accuracy: 0.9912 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0755 - accuracy: 0.9746 - val_loss: 0.0328 - val_accuracy: 0.9912 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0768 - accuracy: 0.9751 - val_loss: 0.0334 - val_accuracy: 0.9899 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0747 - accuracy: 0.9746 - val_loss: 0.0283 - val_accuracy: 0.9921 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0710 - accuracy: 0.9761 - val_loss: 0.0303 - val_accuracy: 0.9914 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0745 - accuracy: 0.9749 - val_loss: 0.0297 - val_accuracy: 0.9915 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0710 - accuracy: 0.9758 - val_loss: 0.0300 - val_accuracy: 0.9912 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0740 - accuracy: 0.9751 - val_loss: 0.0330 - val_accuracy: 0.9902 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0758 - accuracy: 0.9744 - val_loss: 0.0287 - val_accuracy: 0.9915 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0693 - accuracy: 0.9762 - val_loss: 0.0318 - val_accuracy: 0.9906 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0735 - accuracy: 0.9747 - val_loss: 0.0308 - val_accuracy: 0.9910 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0688 - accuracy: 0.9766 - val_loss: 0.0347 - val_accuracy: 0.9895 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0706 - accuracy: 0.9769 - val_loss: 0.0293 - val_accuracy: 0.9917 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0767 - accuracy: 0.9738 - val_loss: 0.0431 - val_accuracy: 0.9863 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0708 - accuracy: 0.9759 - val_loss: 0.0322 - val_accuracy: 0.9904 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0695 - accuracy: 0.9767 - val_loss: 0.0262 - val_accuracy: 0.9922 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0710 - accuracy: 0.9756 - val_loss: 0.0382 - val_accuracy: 0.9891 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0732 - accuracy: 0.9758 - val_loss: 0.0297 - val_accuracy: 0.9910 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0734 - accuracy: 0.9755 - val_loss: 0.0294 - val_accuracy: 0.9914 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0714 - accuracy: 0.9755 - val_loss: 0.0335 - val_accuracy: 0.9902 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0724 - accuracy: 0.9755 - val_loss: 0.0289 - val_accuracy: 0.9917 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 - 10s - loss: 0.0716 - accuracy: 0.9763 - val_loss: 0.0305 - val_accuracy: 0.9914 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 200\n",
    "\n",
    "history = model.fit_generator(train_augmentation.flow(X_train_LeNet5, Y_train_LeNet5, batch_size=batch_size),\n",
    "                              steps_per_epoch=len(X_train_LeNet5)//batch_size,\n",
    "                              epochs=epochs,\n",
    "                              validation_data=(X_valid_LeNet5, Y_valid_LeNet5),\n",
    "                              validation_steps=50,\n",
    "                              callbacks=[learning_rate_reduction, es],\n",
    "                              verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjQVca22WYfM"
   },
   "source": [
    "##### ***Evaluating the Model***\n",
    "\n",
    "From the learning curve graph below, It can be seen that the accuracy of both training and validation set converge and the accuracy was improve during the training of the model. As a reult, the validation accuracy is 99.14%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "7kGblq4EnPEK",
    "outputId": "ccbefce6-b74b-4913-be5c-e4261249c598"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAGFCAYAAADU/MRFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOXTQ53wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Learning curve\n",
    "pd.DataFrame(history.history).plot(figsize=(12, 6))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title('Learning Curve', fontsize=15)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_TrhYYeUkWvV"
   },
   "source": [
    "##### ***Making prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "HEAasgFU42a6",
    "outputId": "9ab7fb9f-7011-40be-f524-81e7e3e21837"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 9])"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test = model.predict_classes(X_test_LeNet5)\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5s5cIcXLA57O"
   },
   "source": [
    "#### **3.3.5 CNN Architecture Improvement**\n",
    "\n",
    "The architecture that generated the highest accuracy that we found will be provided in the following. This model was taken from [25 Million Images! [0.99757] MNIST](https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist) in Kaggle.\n",
    "\n",
    "This simple architecture starts with convolution layers, then follow by a pooling layer. Then another convolution layers and pooling were added. Then, finish with a dense layer and a SoftMax layer. There are some unique features in a model that can help improve the accuracy, that is pooling layers batch normalisation, and dropout. Batch normalisation would be added after the convolution layers to normalise the input of a hidden layer by scaling the activations. This can help to increase the training speed and regularise the model. Moreover, dropout technique is the powerful regularisation technique, which would be added to prevent the over-fitting problem. Therefore, for the model in this project, a number of the filter size, padding type and parameters of pooling, batch normalisation, dropout, activation function type will be adjusted by trial and error to maximise the accuracy of the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 814
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "w4DpGBFAA5Jm",
    "outputId": "02d413b3-b13b-43eb-c2df-d556b3a27bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 26, 26, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 10, 10, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 1,301,642\n",
      "Trainable params: 1,299,978\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Perform layers\n",
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "    tf.keras.layers.Conv2D(64,kernel_size=3,activation='relu',input_shape=(28,28,1)),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
    "    tf.keras.layers.Conv2D(64,kernel_size=3,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
    "    tf.keras.layers.Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "\n",
    "    tf.keras.layers.Conv2D(128,kernel_size=3,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
    "    tf.keras.layers.Conv2D(128,kernel_size=3,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
    "    tf.keras.layers.Conv2D(128,kernel_size=5,strides=2,padding='same',activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Displays all model's layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywg_pjt4DHNR"
   },
   "source": [
    "##### ***Compiling and Fitting Model***\n",
    "In compile model, we used the loss function as â€œcategorical_crossentropyâ€, metric to evaluate model is â€œaccuracyâ€ and the optimiser would be â€œRMSpropâ€.\n",
    "\n",
    "We have already experimented that RMSprop performs better than Adam and SGD optimiser. Adam optimiser learn faster, reach the minimum point faster, but gave the lower validation accuracy. While SGD seems does not work well with this model because it cannot reach the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Jc0GXekyFyCU"
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(learning_rate=0.001, rho=0.9)  # or learning_rate = 0.0025\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hasmr_y0ITWA"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 200\n",
    "\n",
    "history = model.fit_generator(train_augmentation .flow(X_train_i, Y_train_i, batch_size=batch_size),\n",
    "                              steps_per_epoch=len(X_train_i)//batch_size,\n",
    "                              epochs=epochs,\n",
    "                              validation_data=(X_valid_i, Y_valid_i),\n",
    "                              validation_steps=50,\n",
    "                              callbacks=[learning_rate_reduction, es],\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PoiRhT-yWfp3"
   },
   "source": [
    "##### ***Evaluating the Model***\n",
    "\n",
    "From the learning curve graph below, It can be seen that the accuracy of both training and validation set converge to 1. This means the accuracy was improve during the training of the model. As a reult, the validation accuracy is 99.52%, which is our best score that we obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "W9E-dJfEIWyz"
   },
   "outputs": [],
   "source": [
    "# Plot Learning curve\n",
    "pd.DataFrame(history.history).plot(figsize=(12, 6))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title('Learning Curve', fontsize=15)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1U6_GS4ikayf"
   },
   "source": [
    "##### ***Making prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "s6TLtXkMuGz1",
    "outputId": "34b76ba1-ef5c-4cab-f8f3-292b11159134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-44-8f3ee4a7503b>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 9])"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test = model.predict_classes(X_test_i)\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "33JACTj3HNdm"
   },
   "source": [
    "# **4. Conclusions**\n",
    "\n",
    "This task will train the model to predict the number of Kannada MNIST dataset using Deep Learning techniques. Fully Connected Network and Convolutional Neural Network (CNN) approach will be implemented. The benefit of the CNN is there are parameters less than Fully connected network method that can train a model faster.\n",
    "\n",
    "In the fully connected network part, we obtained the outcomes that Adam optimiser learns faster than SGD, but SGD gives higher accuracy. Besides, a model regularisation can improve the efficiency of the model using dropout technique.\n",
    "\n",
    "In the CNN approach, a proper architecture could contribute the better performance. Data augmentation is one of the most crucial techniques that affect the performance of the model. Moreover, the size of convolution layer, pooling layer, activation function, batch normalisation, decaying learning rate and dropout have an impact on the accuracy as well.\n",
    "\n",
    "As a result, CNN performs outstanding accuracy, around 99.5%, while the highest accuracy of the Fully Connected network is approximately 98%.\n",
    "\n",
    "We hope that this task will help you more understanding about Deep Learning in practical. We are exciting when we tried different parameters. Our suggestion is try to change other settings such as the number of filters, activation function type, and other parameters to build the model and observe the outcomes.\n",
    "\n",
    "The following section is the summarise of our code that provided the best accuracy, and it will be submitted to Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuJc5BnZIyC9"
   },
   "source": [
    "# **5. References**\n",
    "\n",
    "- Heaton, J. (2015). AIFH, Volume 3: Deep Learning and Neural Networks.\n",
    "- LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D8HH054LaLTR"
   },
   "source": [
    "# **Submission Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "d3_Td2jGcduN",
    "outputId": "58613306-55ed-4570-f2a3-c0187df24017"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-f4ca830a-499e-4b1c-84d4-38078ade50d0\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-f4ca830a-499e-4b1c-84d4-38078ade50d0\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle (1).json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kaggle.json': b'{\"username\":\"chinnakritn\",\"key\":\"1e82857533b2b1c8622562ac08a53ea1\"}'}"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "tku0WaqrdJMn"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/kaggle.json\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9TWT4fJjdfdh"
   },
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import numpy as np    # Implemennts milti-dimensional array and matrices\n",
    "import pandas as pd  # for data manipulation and analysis\n",
    "import tensorflow as tf # for create deep learning models\n",
    "from tensorflow import keras # implementation of the Keras API specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "gN274NYyIyDA",
    "outputId": "e9e9cde5-5d40-4042-c719-cd67f8a35736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.2.0-rc2\n",
      "Keras version 2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "# To check version of tensorflow and keras\n",
    "print('Tensorflow version',tf.__version__)\n",
    "print('Keras version',keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "hBfikNl6aXwA",
    "outputId": "f139c8bf-6d30-415a-9ce5-0b9f1b13487c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
      "Downloading test.csv.zip to /content\n",
      "\r",
      "  0% 0.00/1.79M [00:00<?, ?B/s]\n",
      "100% 1.79M/1.79M [00:00<00:00, 60.1MB/s]\n",
      "Downloading training.csv.zip to /content\n",
      " 50% 5.00M/9.92M [00:00<00:00, 34.8MB/s]\n",
      "100% 9.92M/9.92M [00:00<00:00, 46.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c cs98x-kannada-mnist --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "LgMmhxUDaX1U"
   },
   "outputs": [],
   "source": [
    "# load training & test datasets\n",
    "train = pd.read_csv(\"/content/training.csv.zip\")\n",
    "test = pd.read_csv(\"/content/test.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hKYCnxPin_NL"
   },
   "outputs": [],
   "source": [
    "# Create a copy of data that does not affect to original trian and test set\n",
    "train_i = train.copy()\n",
    "test_i = test.copy()\n",
    "\n",
    "X_i = train_i.values[:,2:]            # Segment train output variables\n",
    "Y_i = train_i.loc[:,'label'].values   # Segment train input variables\n",
    "X_testing_i = test_i.values[:,1:]        # Segment test input variables\n",
    "\n",
    "X_i = np.array(X_i).reshape(60000,28, 28)\n",
    "X_testing_i = np.array(X_testing_i).reshape(10000,28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2iSUmtQboAAD"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sklearn   # import tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction\n",
    "from sklearn import model_selection, preprocessing  # import model selection and preprocessing\n",
    "from sklearn.preprocessing import scale  # import data normalise tool\n",
    "from sklearn.model_selection import train_test_split   # to divide dataset into train and test set\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GkZCiA8pn_62"
   },
   "outputs": [],
   "source": [
    "# Spliting train and validation set into 70:30 ratio\n",
    "X_train_i, X_valid_i, Y_train_i, Y_valid_i = sklearn.model_selection.train_test_split(X_i, Y_i, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "ugxqZNF7n_Ge",
    "outputId": "a932872c-1bd8-4402-9a5b-8b47b46a0a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (42000, 28, 28, 1)\n",
      "42000 train samples\n",
      "18000 validation samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# data preparation\n",
    "img_rows, img_cols = X_i.shape[1], X_i.shape[1]\n",
    "\n",
    "# Reshape the image \n",
    "X_train_i = X_train_i.reshape(X_train_i.shape[0], img_rows, img_cols, 1)\n",
    "X_valid_i = X_valid_i.reshape(X_valid_i.shape[0], img_rows, img_cols, 1)\n",
    "X_test_i = X_testing_i.reshape(X_testing_i.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)  \n",
    "\n",
    "X_train_i = X_train_i.astype('float32')\n",
    "X_valid_i = X_valid_i.astype('float32')\n",
    "X_test_i = X_test_i.astype('float32')\n",
    "\n",
    "# Normalisation scale of X_train and X_valid\n",
    "X_train_i = X_train_i/255\n",
    "X_valid_i = X_valid_i/255\n",
    "X_test_i = X_test_i/255\n",
    "Y_train_i = Y_train_i\n",
    "Y_valid_i = Y_valid_i\n",
    "print('x_train shape:', X_train_i.shape)\n",
    "print(X_train_i.shape[0], 'train samples')\n",
    "print(X_valid_i.shape[0], 'validation samples')\n",
    "print(X_test_i.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "Y_train_i = keras.utils.to_categorical(Y_train_i, num_classes)\n",
    "Y_valid_i = keras.utils.to_categorical(Y_valid_i, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CbyZBArRn-_f"
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "    tf.keras.layers.Conv2D(128,kernel_size=3,activation='relu',input_shape=(28,28,1)),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
    "    #tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "    tf.keras.layers.Conv2D(128,kernel_size=3,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
    "    #tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "    tf.keras.layers.Conv2D(128,kernel_size=5,strides=2,padding='same',activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
    "    #tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "\n",
    "    tf.keras.layers.Conv2D(256,kernel_size=3,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
    "    #tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "    tf.keras.layers.Conv2D(256,kernel_size=3,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
    "    #tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "    tf.keras.layers.Conv2D(256,kernel_size=5,strides=2,padding='same',activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=\"uniform\"),\n",
    "    #tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    #tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "    \n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.001, rho=0.9)  # or learning_rate = 0.0025\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kQnrlIvmlD89"
   },
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_augmentation = ImageDataGenerator(rotation_range = 10,\n",
    "                                   width_shift_range = 0.25,\n",
    "                                   height_shift_range = 0.25,\n",
    "                                   shear_range = 0.1,\n",
    "                                   zoom_range = 0.25,\n",
    "                                   horizontal_flip = False)\n",
    "valid_augmentation = ImageDataGenerator() \n",
    "\n",
    "# Early stopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=300, restore_best_weights=True)\n",
    "\n",
    "# Set a learning rate reduction\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "TbFC-38Qs_RX",
    "outputId": "2b542598-84bd-4804-c242-d4e7582d8b9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.5343 - accuracy: 0.8251WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 62ms/step - loss: 0.5343 - accuracy: 0.8251 - val_loss: 0.1945 - val_accuracy: 0.9576 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.1646 - accuracy: 0.9500WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 62ms/step - loss: 0.1646 - accuracy: 0.9500 - val_loss: 0.0610 - val_accuracy: 0.9843 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.1273 - accuracy: 0.9601WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.1273 - accuracy: 0.9601 - val_loss: 0.0415 - val_accuracy: 0.9890 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.1127 - accuracy: 0.9659WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.1127 - accuracy: 0.9659 - val_loss: 0.0402 - val_accuracy: 0.9905 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.9706WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0951 - accuracy: 0.9706 - val_loss: 0.0418 - val_accuracy: 0.9896 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0893 - accuracy: 0.9724WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0893 - accuracy: 0.9724 - val_loss: 0.0465 - val_accuracy: 0.9878 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0824 - accuracy: 0.9760WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 19s 59ms/step - loss: 0.0824 - accuracy: 0.9760 - val_loss: 0.0323 - val_accuracy: 0.9917 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9777WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0750 - accuracy: 0.9777 - val_loss: 0.0250 - val_accuracy: 0.9929 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9782WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 19s 59ms/step - loss: 0.0746 - accuracy: 0.9782 - val_loss: 0.0264 - val_accuracy: 0.9923 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9793WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0671 - accuracy: 0.9793 - val_loss: 0.0411 - val_accuracy: 0.9891 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9787WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0709 - accuracy: 0.9787 - val_loss: 0.0326 - val_accuracy: 0.9923 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9804WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 19s 59ms/step - loss: 0.0662 - accuracy: 0.9804 - val_loss: 0.0249 - val_accuracy: 0.9932 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9798WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0654 - accuracy: 0.9798 - val_loss: 0.0228 - val_accuracy: 0.9939 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9827WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 19s 59ms/step - loss: 0.0586 - accuracy: 0.9827 - val_loss: 0.0307 - val_accuracy: 0.9924 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9824WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0594 - accuracy: 0.9824 - val_loss: 0.0255 - val_accuracy: 0.9937 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9831WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0593 - accuracy: 0.9831 - val_loss: 0.0217 - val_accuracy: 0.9943 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.9831WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 19s 59ms/step - loss: 0.0561 - accuracy: 0.9831 - val_loss: 0.0221 - val_accuracy: 0.9936 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9824WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0578 - accuracy: 0.9824 - val_loss: 0.0244 - val_accuracy: 0.9931 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9831WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0558 - accuracy: 0.9831 - val_loss: 0.0224 - val_accuracy: 0.9937 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9841WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 19s 59ms/step - loss: 0.0528 - accuracy: 0.9841 - val_loss: 0.0194 - val_accuracy: 0.9949 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9843WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0536 - accuracy: 0.9843 - val_loss: 0.0190 - val_accuracy: 0.9949 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9846WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0531 - accuracy: 0.9846 - val_loss: 0.0222 - val_accuracy: 0.9937 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9846WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0494 - accuracy: 0.9846 - val_loss: 0.0204 - val_accuracy: 0.9943 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9843WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0500 - accuracy: 0.9843 - val_loss: 0.0280 - val_accuracy: 0.9919 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9863WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0470 - accuracy: 0.9863 - val_loss: 0.0229 - val_accuracy: 0.9941 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9855WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0490 - accuracy: 0.9855 - val_loss: 0.0375 - val_accuracy: 0.9897 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9855WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 19s 59ms/step - loss: 0.0477 - accuracy: 0.9855 - val_loss: 0.0200 - val_accuracy: 0.9946 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9867WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0438 - accuracy: 0.9867 - val_loss: 0.0262 - val_accuracy: 0.9936 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9854WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0486 - accuracy: 0.9854 - val_loss: 0.0194 - val_accuracy: 0.9946 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9862WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 59ms/step - loss: 0.0476 - accuracy: 0.9862 - val_loss: 0.0150 - val_accuracy: 0.9957 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9869WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 19s 59ms/step - loss: 0.0439 - accuracy: 0.9869 - val_loss: 0.0178 - val_accuracy: 0.9950 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9867WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 19s 59ms/step - loss: 0.0430 - accuracy: 0.9867 - val_loss: 0.0162 - val_accuracy: 0.9956 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9874WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0426 - accuracy: 0.9874 - val_loss: 0.0204 - val_accuracy: 0.9945 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9866WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 61ms/step - loss: 0.0444 - accuracy: 0.9866 - val_loss: 0.0183 - val_accuracy: 0.9956 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9874WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0421 - accuracy: 0.9874 - val_loss: 0.0159 - val_accuracy: 0.9958 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9870WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0427 - accuracy: 0.9870 - val_loss: 0.0169 - val_accuracy: 0.9958 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9872WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0432 - accuracy: 0.9872 - val_loss: 0.0192 - val_accuracy: 0.9954 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9882WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 19s 59ms/step - loss: 0.0402 - accuracy: 0.9882 - val_loss: 0.0250 - val_accuracy: 0.9941 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9870WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0447 - accuracy: 0.9870 - val_loss: 0.0171 - val_accuracy: 0.9954 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9873WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0448 - accuracy: 0.9873 - val_loss: 0.0221 - val_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9878WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0403 - accuracy: 0.9878 - val_loss: 0.0190 - val_accuracy: 0.9945 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9873WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0403 - accuracy: 0.9873 - val_loss: 0.0185 - val_accuracy: 0.9954 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9882WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0405 - accuracy: 0.9882 - val_loss: 0.0205 - val_accuracy: 0.9946 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9872WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0413 - accuracy: 0.9872 - val_loss: 0.0177 - val_accuracy: 0.9952 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9880WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0400 - accuracy: 0.9880 - val_loss: 0.0295 - val_accuracy: 0.9912 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9883WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0424 - accuracy: 0.9883 - val_loss: 0.0187 - val_accuracy: 0.9955 - lr: 0.0010\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9880WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0412 - accuracy: 0.9880 - val_loss: 0.0165 - val_accuracy: 0.9958 - lr: 0.0010\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9877WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0386 - accuracy: 0.9877 - val_loss: 0.0158 - val_accuracy: 0.9961 - lr: 0.0010\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9881WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 61ms/step - loss: 0.0377 - accuracy: 0.9881 - val_loss: 0.0174 - val_accuracy: 0.9951 - lr: 0.0010\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9885WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 61ms/step - loss: 0.0373 - accuracy: 0.9885 - val_loss: 0.0170 - val_accuracy: 0.9959 - lr: 0.0010\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9891WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 59ms/step - loss: 0.0352 - accuracy: 0.9891 - val_loss: 0.0178 - val_accuracy: 0.9957 - lr: 0.0010\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9889WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0367 - accuracy: 0.9889 - val_loss: 0.0177 - val_accuracy: 0.9956 - lr: 0.0010\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9880WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0379 - accuracy: 0.9880 - val_loss: 0.0192 - val_accuracy: 0.9958 - lr: 0.0010\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9888WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0366 - accuracy: 0.9888 - val_loss: 0.0201 - val_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9891WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0369 - accuracy: 0.9891 - val_loss: 0.0192 - val_accuracy: 0.9947 - lr: 0.0010\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9893WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0351 - accuracy: 0.9893 - val_loss: 0.0171 - val_accuracy: 0.9959 - lr: 0.0010\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9891WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0342 - accuracy: 0.9891 - val_loss: 0.0217 - val_accuracy: 0.9951 - lr: 0.0010\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9884WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0364 - accuracy: 0.9884 - val_loss: 0.0190 - val_accuracy: 0.9953 - lr: 0.0010\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9887WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0387 - accuracy: 0.9887 - val_loss: 0.0280 - val_accuracy: 0.9923 - lr: 0.0010\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9895WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0340 - accuracy: 0.9895 - val_loss: 0.0179 - val_accuracy: 0.9957 - lr: 0.0010\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9889WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0357 - accuracy: 0.9889 - val_loss: 0.0263 - val_accuracy: 0.9941 - lr: 0.0010\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9887WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0380 - accuracy: 0.9887 - val_loss: 0.0169 - val_accuracy: 0.9958 - lr: 0.0010\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9891WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0370 - accuracy: 0.9891 - val_loss: 0.0185 - val_accuracy: 0.9956 - lr: 0.0010\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9897WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0332 - accuracy: 0.9897 - val_loss: 0.0197 - val_accuracy: 0.9955 - lr: 0.0010\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9894WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 61ms/step - loss: 0.0353 - accuracy: 0.9894 - val_loss: 0.0271 - val_accuracy: 0.9942 - lr: 0.0010\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9894WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0357 - accuracy: 0.9894 - val_loss: 0.0155 - val_accuracy: 0.9959 - lr: 0.0010\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9896WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0335 - accuracy: 0.9896 - val_loss: 0.0170 - val_accuracy: 0.9957 - lr: 0.0010\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9892WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 61ms/step - loss: 0.0352 - accuracy: 0.9892 - val_loss: 0.0170 - val_accuracy: 0.9957 - lr: 0.0010\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9894WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0358 - accuracy: 0.9894 - val_loss: 0.0181 - val_accuracy: 0.9958 - lr: 0.0010\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9889WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 19s 59ms/step - loss: 0.0356 - accuracy: 0.9889 - val_loss: 0.0171 - val_accuracy: 0.9956 - lr: 0.0010\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9903WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0323 - accuracy: 0.9903 - val_loss: 0.0216 - val_accuracy: 0.9945 - lr: 0.0010\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9906WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0330 - accuracy: 0.9906 - val_loss: 0.0158 - val_accuracy: 0.9955 - lr: 0.0010\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9901WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0345 - accuracy: 0.9901 - val_loss: 0.0181 - val_accuracy: 0.9959 - lr: 0.0010\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9905WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0304 - accuracy: 0.9905 - val_loss: 0.0189 - val_accuracy: 0.9953 - lr: 0.0010\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9903WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 60ms/step - loss: 0.0317 - accuracy: 0.9903 - val_loss: 0.0151 - val_accuracy: 0.9962 - lr: 0.0010\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9895WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 61ms/step - loss: 0.0351 - accuracy: 0.9895 - val_loss: 0.0183 - val_accuracy: 0.9962 - lr: 0.0010\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9907WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 61ms/step - loss: 0.0331 - accuracy: 0.9907 - val_loss: 0.0159 - val_accuracy: 0.9955 - lr: 0.0010\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9905WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 21s 63ms/step - loss: 0.0327 - accuracy: 0.9905 - val_loss: 0.0171 - val_accuracy: 0.9959 - lr: 0.0010\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9900WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 62ms/step - loss: 0.0345 - accuracy: 0.9900 - val_loss: 0.0155 - val_accuracy: 0.9962 - lr: 0.0010\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9901WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 21s 64ms/step - loss: 0.0344 - accuracy: 0.9901 - val_loss: 0.0164 - val_accuracy: 0.9954 - lr: 0.0010\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9906WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 21s 64ms/step - loss: 0.0323 - accuracy: 0.9906 - val_loss: 0.0143 - val_accuracy: 0.9966 - lr: 0.0010\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9906WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 62ms/step - loss: 0.0315 - accuracy: 0.9906 - val_loss: 0.0208 - val_accuracy: 0.9956 - lr: 0.0010\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9901WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 21s 63ms/step - loss: 0.0329 - accuracy: 0.9901 - val_loss: 0.0170 - val_accuracy: 0.9963 - lr: 0.0010\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9897WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 62ms/step - loss: 0.0336 - accuracy: 0.9897 - val_loss: 0.0238 - val_accuracy: 0.9949 - lr: 0.0010\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9907WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 62ms/step - loss: 0.0298 - accuracy: 0.9907 - val_loss: 0.0211 - val_accuracy: 0.9948 - lr: 0.0010\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9914WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 62ms/step - loss: 0.0309 - accuracy: 0.9914 - val_loss: 0.0317 - val_accuracy: 0.9943 - lr: 0.0010\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9908WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 62ms/step - loss: 0.0304 - accuracy: 0.9908 - val_loss: 0.0179 - val_accuracy: 0.9963 - lr: 0.0010\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9907WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 62ms/step - loss: 0.0329 - accuracy: 0.9907 - val_loss: 0.0153 - val_accuracy: 0.9963 - lr: 0.0010\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9907WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 21s 63ms/step - loss: 0.0309 - accuracy: 0.9907 - val_loss: 0.0195 - val_accuracy: 0.9957 - lr: 0.0010\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9909WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 21s 63ms/step - loss: 0.0299 - accuracy: 0.9909 - val_loss: 0.0182 - val_accuracy: 0.9957 - lr: 0.0010\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9915WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 21s 63ms/step - loss: 0.0288 - accuracy: 0.9915 - val_loss: 0.0142 - val_accuracy: 0.9962 - lr: 0.0010\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9910WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 62ms/step - loss: 0.0282 - accuracy: 0.9910 - val_loss: 0.0533 - val_accuracy: 0.9936 - lr: 0.0010\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9913WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 61ms/step - loss: 0.0292 - accuracy: 0.9913 - val_loss: 0.0263 - val_accuracy: 0.9947 - lr: 0.0010\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9901WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 61ms/step - loss: 0.0332 - accuracy: 0.9901 - val_loss: 0.0172 - val_accuracy: 0.9952 - lr: 0.0010\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9911WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 61ms/step - loss: 0.0313 - accuracy: 0.9911 - val_loss: 0.0193 - val_accuracy: 0.9958 - lr: 0.0010\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9916WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 62ms/step - loss: 0.0291 - accuracy: 0.9916 - val_loss: 0.0161 - val_accuracy: 0.9964 - lr: 0.0010\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9913WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 61ms/step - loss: 0.0285 - accuracy: 0.9913 - val_loss: 0.0193 - val_accuracy: 0.9954 - lr: 0.0010\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9908WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 61ms/step - loss: 0.0301 - accuracy: 0.9908 - val_loss: 0.0171 - val_accuracy: 0.9956 - lr: 0.0010\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9914WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 61ms/step - loss: 0.0293 - accuracy: 0.9914 - val_loss: 0.0155 - val_accuracy: 0.9960 - lr: 0.0010\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9915WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "328/328 [==============================] - 20s 61ms/step - loss: 0.0280 - accuracy: 0.9915 - val_loss: 0.0176 - val_accuracy: 0.9960 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "history = model.fit_generator(train_augmentation .flow(X_train_i, Y_train_i, batch_size=batch_size),\n",
    "                              steps_per_epoch=len(X_train_i)//batch_size,\n",
    "                              epochs=epochs,\n",
    "                              validation_data=(X_valid_i, Y_valid_i),\n",
    "                              validation_steps=50,\n",
    "                              callbacks=[learning_rate_reduction, es],\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8vZ2KS8Z1kGn"
   },
   "source": [
    "To sum up, This model could give the bast accuracy at around 99.57% of validation set. However, when it was submitted to the Kaggle Competition \"CS98X Kannada MNIST\", it provided the accuracy score as 0.9840 or 98.40% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IDT3nlNhn-5s",
    "outputId": "462270bf-b760-4a73-e4b5-271b51fa4691"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 9])"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the testing set\n",
    "predictions_test = model.predict_classes(X_test_i)\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqNHLR8FDhaw"
   },
   "source": [
    "# **Submission Area**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bh5uKgojnwTK"
   },
   "outputs": [],
   "source": [
    "test1 = test.copy()\n",
    "test1[\"label\"] = predictions_test\n",
    "\n",
    "submission = test1[[\"id\",\"label\"]]\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "u6P9ukU7nwPu",
    "outputId": "928497d8-1fb0-495f-9e69-fb6818364be7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
      "\r",
      "  0% 0.00/78.1k [00:00<?, ?B/s]\r",
      "100% 78.1k/78.1k [00:03<00:00, 20.9kB/s]\n",
      "Successfully submitted to CS98X Kannada MNIST"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c cs98x-kannada-mnist -f submission.csv -m \"Message\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "qUT0_c-bHNcI",
    "2y9jwSd8HNcx"
   ],
   "name": "Group O_CS985 and CS987_Kannada-MNIST Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
